import streamlit as st
from langchain_community.document_loaders import PyPDFLoader, Docx2txtLoader, TextLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings
from langchain.chains import ConversationalRetrievalChain
import requests
import json
from langchain.memory import ConversationBufferWindowMemory
from langchain.prompts import PromptTemplate
import os
import pickle
import json
from datetime import datetime
import requests
import tempfile
import glob
from pathlib import Path
from dotenv import load_dotenv
import base64
import warnings, logging
warnings.filterwarnings("ignore")
logging.getLogger().setLevel(logging.ERROR)
debug = False
verbose = False


def get_base64_of_image(path):
    """Convert image to base64 string"""
    try:
        with open(path, "rb") as img_file:
            return base64.b64encode(img_file.read()).decode()
    except Exception as e:
        return ""

# C·∫•u h√¨nh trang
st.set_page_config(
    page_title="Chatbot T∆∞ V·∫•n - ƒê·∫°i h·ªçc Lu·∫≠t TPHCM",
    page_icon="‚öñÔ∏è",
    layout="wide",
    initial_sidebar_state="expanded"
)

# CSS t·ªëi ∆∞u
st.markdown("""
<style>
.header-container {
    background: linear-gradient(90deg, #003366, #004c99);
    color: white;
    padding: 24px 0;
    border-radius: 16px;
    text-align: center;
    box-shadow: 0 4px 15px rgba(0,0,0,0.25);
    margin-bottom: 20px;
}
.header-container h1 {
    font-size: 2rem;
    margin-bottom: 0.4rem;
    font-weight: 700;
}
.header-container h3 {
    font-size: 1.2rem;
    font-weight: 400;
    margin-top: 0;
    opacity: 0.9;
}
.header-container p {
    font-size: 1rem;
    margin-top: 0.2rem;
    opacity: 0.85;
}
</style>

<div class="header-container">
    <h1>ü§ñ Chatbot T∆∞ V·∫•n Tuy·ªÉn Sinh</h1>
    <h3>Tr∆∞·ªùng ƒê·∫°i h·ªçc Lu·∫≠t TP. H·ªì Ch√≠ Minh</h3>
    <p>üí¨ H·ªó tr·ª£ 24/7 &nbsp; | &nbsp; üéì T∆∞ v·∫•n chuy√™n nghi·ªáp</p>
</div>
""", unsafe_allow_html=True)


# Load bi·∫øn m√¥i tr∆∞·ªùng
load_dotenv()

# ƒê·ªåC API KEY T·ª™ SECRETS TR∆Ø·ªöC, SAU ƒê√ì M·ªöI ƒê·∫æN .env
try:
    gemini_api_key = st.secrets["GEMINI_API_KEY"]
except:
    gemini_api_key = os.getenv("GEMINI_API_KEY")

try:
    GDRIVE_VECTORSTORE_ID = st.secrets["GDRIVE_VECTORSTORE_ID"]
except:
    GDRIVE_VECTORSTORE_ID = os.getenv("GDRIVE_VECTORSTORE_ID")

try:
    GDRIVE_METADATA_ID = st.secrets["GDRIVE_METADATA_ID"]
except:
    GDRIVE_METADATA_ID = os.getenv("GDRIVE_METADATA_ID")

# C·∫•u h√¨nh ƒë∆∞·ªùng d·∫´n
DOCUMENTS_PATH = "documents"
VECTORSTORE_PATH = "vectorstore"

for path in [DOCUMENTS_PATH, VECTORSTORE_PATH]:
    Path(path).mkdir(exist_ok=True)

# Template prompt
COUNSELING_PROMPT_TEMPLATE = """
B·∫°n l√† chuy√™n gia t∆∞ v·∫•n tuy·ªÉn sinh Tr∆∞·ªùng ƒê·∫°i h·ªçc Lu·∫≠t Th√†nh ph·ªë H·ªì Ch√≠ Minh.

TH√îNG TIN LI√äN H·ªÜ CH√çNH TH·ª®C:
- Hotline tuy·ªÉn sinh: 1900 5555 14 ho·∫∑c 0879 5555 14
- Email: tuyensinh@hcmulaw.edu.vn
- ƒêi·ªán tho·∫°i: (028) 39400 989
- ƒê·ªãa ch·ªâ: 2 Nguy·ªÖn T·∫•t Th√†nh, Ph∆∞·ªùng 12, Qu·∫≠n 4, TP.HCM
- Website: www.hcmulaw.edu.vn

Nguy√™n t·∫Øc tr·∫£ l·ªùi:
1. Th√¢n thi·ªán, chuy√™n nghi·ªáp
2. Cung c·∫•p th√¥ng tin ch√≠nh x√°c v·ªÅ ƒê·∫°i h·ªçc Lu·∫≠t TPHCM
3. KH√îNG s·ª≠ d·ª•ng placeholder, lu√¥n d√πng th√¥ng tin li√™n h·ªá c·ª• th·ªÉ ·ªü tr√™n
4. N·∫øu kh√¥ng ch·∫Øc ch·∫Øn, khuy·∫øn kh√≠ch li√™n h·ªá tr·ª±c ti·∫øp

Th√¥ng tin tham kh·∫£o: {context}
L·ªãch s·ª≠ h·ªôi tho·∫°i: {chat_history}
C√¢u h·ªèi: {question}

Tr·∫£ l·ªùi (ti·∫øng Vi·ªát):
"""

@st.cache_resource
def load_embeddings():
    return HuggingFaceEmbeddings(
        model_name="keepitreal/vietnamese-sbert",
        model_kwargs={'device': 'cpu'},
        encode_kwargs={'normalize_embeddings': True}
    )

embeddings = load_embeddings()

def download_from_gdrive_direct(file_id, output_path):
    """
    Download file t·ª´ Google Drive b·∫±ng requests (kh√¥ng d√πng gdown)
    File ph·∫£i ƒë∆∞·ª£c share c√¥ng khai: Anyone with the link
    """
    try:
        # URL download tr·ª±c ti·∫øp t·ª´ GDrive
        url = f"https://drive.google.com/uc?export=download&id={file_id}"
        
        session = requests.Session()
        response = session.get(url, stream=True)
        
        # X·ª≠ l√Ω virus scan warning c·ªßa GDrive (file l·ªõn)
        for key, value in response.cookies.items():
            if key.startswith('download_warning'):
                params = {'export': 'download', 'id': file_id, 'confirm': value}
                response = session.get(url, params=params, stream=True)
                break
        
        # L∆∞u file
        with open(output_path, 'wb') as f:
            for chunk in response.iter_content(chunk_size=32768):
                if chunk:
                    f.write(chunk)
        
        return True
        
    except Exception as e:
        #st.warning(f"‚ö†Ô∏è Kh√¥ng th·ªÉ t·∫£i t·ª´ GDrive: {e}")
        return False

def get_document_files():
    """L·∫•y danh s√°ch file trong documents"""
    files = []
    for ext in ['*.pdf', '*.docx', '*.txt']:
        files.extend(glob.glob(os.path.join(DOCUMENTS_PATH, '**', ext), recursive=True))
    return files

def get_file_hash(file_path):
    """T·∫°o hash cho file"""
    stat = os.stat(file_path)
    return f"{stat.st_mtime}_{stat.st_size}"

def load_cached_vectorstore():
    """Load vector store t·ª´ Google Drive"""
    if not GDRIVE_VECTORSTORE_ID or not GDRIVE_METADATA_ID:
        #st.info("‚ÑπÔ∏è Ch∆∞a c·∫•u h√¨nh Google Drive, s·ª≠ d·ª•ng x·ª≠ l√Ω local")
        return None, {}
    
    temp_dir = tempfile.mkdtemp()
    vectorstore_path = os.path.join(temp_dir, "vectorstore.pkl")
    metadata_path = os.path.join(temp_dir, "metadata.json")
    
    try:
        # Download b·∫±ng requests thay v√¨ gdown
        if not download_from_gdrive_direct(GDRIVE_VECTORSTORE_ID, vectorstore_path):
            return None, {}
        if not download_from_gdrive_direct(GDRIVE_METADATA_ID, metadata_path):
            return None, {}
        
        with open(vectorstore_path, 'rb') as f:
            vectorstore = pickle.load(f)
        
        with open(metadata_path, 'r', encoding='utf-8') as f:
            metadata = json.load(f)
        
        # Cleanup
        os.remove(vectorstore_path)
        os.remove(metadata_path)
        os.rmdir(temp_dir)
        
        #st.success("‚úÖ ƒê√£ load vectorstore t·ª´ Google Drive")
        return vectorstore, metadata
        
    except Exception as e:
        st.warning(f"‚ö†Ô∏è Kh√¥ng th·ªÉ load t·ª´ GDrive: {e}")
        # Cleanup n·∫øu c√≥ l·ªói
        try:
            if os.path.exists(vectorstore_path):
                os.remove(vectorstore_path)
            if os.path.exists(metadata_path):
                os.remove(metadata_path)
            if os.path.exists(temp_dir):
                os.rmdir(temp_dir)
        except:
            pass
        return None, {}

def process_documents(file_paths):
    """X·ª≠ l√Ω documents"""
    documents = []
    processed = []
    failed = []
    
    for file_path in file_paths:
        try:
            ext = Path(file_path).suffix.lower()
            
            if ext == ".pdf":
                loader = PyPDFLoader(file_path)
            elif ext == ".docx":
                loader = Docx2txtLoader(file_path)
            elif ext == ".txt":
                loader = TextLoader(file_path, encoding='utf-8')
            else:
                failed.append(f"{file_path} (kh√¥ng h·ªó tr·ª£)")
                continue
            
            docs = loader.load()
            for doc in docs:
                doc.metadata['source_file'] = os.path.basename(file_path)
                doc.metadata['processed_time'] = datetime.now().isoformat()
            
            documents.extend(docs)
            processed.append(file_path)
            
        except Exception as e:
            failed.append(f"{file_path} ({str(e)})")
    
    return documents, processed, failed

def create_vector_store(documents):
    """T·∫°o vector store"""
    if not documents:
        return None
    
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=200,
        separators=['\n\n', '\n', '.', '!', '?', ';', ':', ' ']
    )
    texts = text_splitter.split_documents(documents)
    texts = [t for t in texts if len(t.page_content.strip()) > 50]
    
    if not texts:
        return None
    
    return FAISS.from_documents(texts, embeddings)

@st.cache_resource
def initialize_vectorstore():
    """Kh·ªüi t·∫°o vectorstore"""
    # Th·ª≠ load t·ª´ GDrive tr∆∞·ªõc
    vectorstore, metadata = load_cached_vectorstore()
    if vectorstore:
        return vectorstore, metadata.get('stats', {})
    
    # N·∫øu kh√¥ng c√≥ GDrive, x·ª≠ l√Ω local files
    st.info("‚ÑπÔ∏è ƒêang x·ª≠ l√Ω t√†i li·ªáu local...")
    current_files = get_document_files()
    
    if not current_files:
        st.warning("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y file n√†o trong th∆∞ m·ª•c documents")
        return None, {}
    
    with st.spinner("üîÑ ƒêang x·ª≠ l√Ω t√†i li·ªáu..."):
        documents, processed, failed = process_documents(current_files)
        
        if not documents:
            st.error("‚ùå Kh√¥ng th·ªÉ x·ª≠ l√Ω file n√†o")
            return None, {}
        
        vectorstore = create_vector_store(documents)
        
        if vectorstore:
            stats = {
                'total_files': len(current_files),
                'processed_files': len(processed),
                'failed_files': len(failed),
                'total_chunks': vectorstore.index.ntotal,
                'last_updated': datetime.now().isoformat()
            }
            st.success(f"‚úÖ ƒê√£ x·ª≠ l√Ω {len(processed)} files th√†nh c√¥ng!")
            return vectorstore, stats
    
    return None, {}

def classify_question(question):
    """Ph√¢n lo·∫°i c√¢u h·ªèi"""
    question_lower = question.lower()
    
    categories = {
        "Tuy·ªÉn sinh": ["tuy·ªÉn sinh", "ƒëƒÉng k√Ω", "h·ªì s∆°", "ƒëi·ªÉm chu·∫©n", "x√©t tuy·ªÉn"],
        "H·ªçc ph√≠": ["h·ªçc ph√≠", "chi ph√≠", "mi·ªÖn gi·∫£m", "h·ªçc b·ªïng"],
        "Ch∆∞∆°ng tr√¨nh ƒë√†o t·∫°o": ["ch∆∞∆°ng tr√¨nh", "m√¥n h·ªçc", "t√≠n ch·ªâ", "ng√†nh"],
    }
    
    for category, keywords in categories.items():
        if any(kw in question_lower for kw in keywords):
            return category
    return "Kh√°c"

def get_category_badge(category):
    """T·∫°o badge cho category"""
    badge_map = {
        "Tuy·ªÉn sinh": "badge-tuyensinh",
        "H·ªçc ph√≠": "badge-hocphi",
        "Ch∆∞∆°ng tr√¨nh ƒë√†o t·∫°o": "badge-chuongtrinh"
    }
    badge_class = badge_map.get(category, "badge-tuyensinh")
    return f'<span class="category-badge {badge_class}">{category}</span>'

def create_conversational_chain(vector_store, llm):
    """
    T·∫°o chain v·ªõi Google Generative AI
    Kh√¥ng d√πng LangChain chain n·ªØa - x·ª≠ l√Ω tr·ª±c ti·∫øp
    """
    # Tr·∫£ v·ªÅ tuple: (vectorstore, llm) ƒë·ªÉ x·ª≠ l√Ω manual
    return (vector_store, llm)

@st.cache_resource
def get_gemini_llm():
    """
    T·∫°o Gemini client d√πng REST API tr·ª±c ti·∫øp
    GI·∫¢I PH√ÅP CU·ªêI C√ôNG - LU√îN HO·∫†T ƒê·ªòNG
    """
    if not gemini_api_key:
        st.error("‚ùå Thi·∫øu GEMINI_API_KEY!")
        st.stop()
    
    # Test API key
    test_url = f"https://generativelanguage.googleapis.com/v1/models?key={gemini_api_key}"
    
    try:
        response = requests.get(test_url, timeout=10)
        
        if response.status_code == 200:
            models_data = response.json()
            available_models = [m['name'] for m in models_data.get('models', [])]
            
            # T√¨m model t·ªët nh·∫•t c√≥ s·∫µn
            preferred_models = [
                'models/gemini-1.5-flash-latest',
                'models/gemini-1.5-flash',
                'models/gemini-1.5-pro-latest',
                'models/gemini-1.5-pro',
                'models/gemini-pro'
            ]
            
            selected_model = None
            for model in preferred_models:
                if model in available_models:
                    selected_model = model
                    break
            
            if not selected_model and available_models:
                # N·∫øu kh√¥ng c√≥ model ∆∞a th√≠ch, l·∫•y model ƒë·∫ßu ti√™n
                selected_model = available_models[0]
            
            if selected_model:
                #st.success(f"‚úÖ ƒê√£ k·∫øt n·ªëi Gemini: {selected_model}")
                return {
                    'api_key': gemini_api_key,
                    'model': selected_model,
                    'available_models': available_models
                }
            else:
                st.error("‚ùå Kh√¥ng t√¨m th·∫•y model n√†o!")
                st.stop()
                
        elif response.status_code == 400:
            st.error("‚ùå API key kh√¥ng h·ª£p l·ªá!")
            st.info("L·∫•y API key m·ªõi t·∫°i: https://aistudio.google.com/app/apikey")
            st.stop()
        else:
            st.error(f"‚ùå L·ªói API: {response.status_code}")
            st.stop()
            
    except requests.exceptions.Timeout:
        st.error("‚ùå Timeout khi k·∫øt n·ªëi Gemini API")
        st.stop()
    except Exception as e:
        st.error(f"‚ùå L·ªói: {e}")
        st.info("""
        **H∆∞·ªõng d·∫´n:**
        1. L·∫•y API key: https://aistudio.google.com/app/apikey
        2. Th√™m v√†o Streamlit Secrets:
        ```
        GEMINI_API_KEY = "AIzaSy..."
        ```
        """)
        st.stop()

def call_gemini_api(llm_config, prompt):
    """
    G·ªçi Gemini API b·∫±ng REST API
    """
    api_key = llm_config['api_key']
    model = llm_config['model']
    
    # API endpoint
    url = f"https://generativelanguage.googleapis.com/v1/{model}:generateContent?key={api_key}"
    
    # Request body
    payload = {
        "contents": [{
            "parts": [{"text": prompt}]
        }],
        "generationConfig": {
            "temperature": 0.3,
            "topK": 40,
            "topP": 0.95,
            "maxOutputTokens": 2000,
        }
    }
    
    headers = {"Content-Type": "application/json"}
    
    try:
        response = requests.post(url, json=payload, headers=headers, timeout=30)
        
        if response.status_code == 200:
            data = response.json()
            
            # Extract text t·ª´ response
            if 'candidates' in data and len(data['candidates']) > 0:
                candidate = data['candidates'][0]
                if 'content' in candidate and 'parts' in candidate['content']:
                    parts = candidate['content']['parts']
                    if len(parts) > 0 and 'text' in parts[0]:
                        return parts[0]['text']
            
            return "Xin l·ªói, kh√¥ng nh·∫≠n ƒë∆∞·ª£c ph·∫£n h·ªìi t·ª´ AI."
            
        else:
            error_msg = response.json().get('error', {}).get('message', 'Unknown error')
            return f"L·ªói API: {error_msg}"
            
    except requests.exceptions.Timeout:
        return "L·ªói: Timeout khi g·ªçi API"
    except Exception as e:
        return f"L·ªói: {str(e)}"
def answer_from_external_api(prompt, llm_config, question_category):
    """Tr·∫£ l·ªùi t·ª´ Gemini REST API"""
    enhanced_prompt = f"""
B·∫°n l√† chuy√™n gia t∆∞ v·∫•n {question_category.lower()} c·ªßa ƒê·∫°i h·ªçc Lu·∫≠t TPHCM.

TH√îNG TIN LI√äN H·ªÜ (B·∫ÆT BU·ªòC S·ª¨ D·ª§NG):
- Hotline: 1900 5555 14 ho·∫∑c 0879 5555 14
- Email: tuyensinh@hcmulaw.edu.vn
- ƒêi·ªán tho·∫°i: (028) 39400 989
- ƒê·ªãa ch·ªâ: 2 Nguy·ªÖn T·∫•t Th√†nh, Ph∆∞·ªùng 12, Qu·∫≠n 4, TP.HCM
- Website: www.hcmulaw.edu.vn

C√¢u h·ªèi: {prompt}

QUY T·∫ÆC:
- KH√îNG d√πng placeholder nh∆∞ [S·ªë ƒëi·ªán tho·∫°i], [Email]
- Lu√¥n d√πng th√¥ng tin c·ª• th·ªÉ ·ªü tr√™n
- K·∫øt th√∫c b·∫±ng th√¥ng tin li√™n h·ªá n·∫øu c·∫ßn

Tr·∫£ l·ªùi th√¢n thi·ªán, chuy√™n nghi·ªáp b·∫±ng ti·∫øng Vi·ªát:
"""
    
    try:
        answer = call_gemini_api(llm_config, enhanced_prompt)
        
        # Thay th·∫ø placeholder c√≤n s√≥t
        replacements = {
            "[S·ªë ƒëi·ªán tho·∫°i": "1900 5555 14 ho·∫∑c 0879 5555 14",
            "[Email": "tuyensinh@hcmulaw.edu.vn",
            "[Website": "www.hcmulaw.edu.vn",
            "[ƒêi·ªán tho·∫°i": "(028) 39400 989"
        }
        
        for placeholder, actual in replacements.items():
            if placeholder in answer:
                answer = answer.replace(placeholder + "]", actual)
        
        return answer
        
    except Exception as e:
        return f"""
Xin l·ªói, h·ªá th·ªëng g·∫∑p s·ª± c·ªë. Vui l√≤ng li√™n h·ªá:

üìû **Hotline:** 1900 5555 14 ho·∫∑c 0879 5555 14
üìß **Email:** tuyensinh@hcmulaw.edu.vn
üåê **Website:** www.hcmulaw.edu.vn
üìç **ƒê·ªãa ch·ªâ:** 2 Nguy·ªÖn T·∫•t Th√†nh, P.12, Q.4, TP.HCM

_(L·ªói k·ªπ thu·∫≠t: {str(e)[:100]})_
"""

def display_quick_questions():
    """Hi·ªÉn th·ªã c√¢u h·ªèi g·ª£i √Ω"""
    st.markdown("### üí° C√¢u h·ªèi th∆∞·ªùng g·∫∑p")
    
    questions = [
        "üìù Th·ªß t·ª•c ƒëƒÉng k√Ω x√©t tuy·ªÉn?",
        "üí∞ H·ªçc ph√≠ c·ªßa tr∆∞·ªùng?",
        "üìö C√°c ng√†nh h·ªçc?",
        "üè† Tr∆∞·ªùng c√≥ k√Ω t√∫c x√° kh√¥ng?",
        "üéì C∆° h·ªôi vi·ªác l√†m?",
        "üìû Th√¥ng tin li√™n h·ªá?"
    ]
    
    for i, q in enumerate(questions):
        if st.button(q, key=f"q_{i}"):
            st.session_state["pending_question"] = q
            st.rerun()

def main():
    # Ki·ªÉm tra API key
    if not gemini_api_key:
        st.error("‚ö†Ô∏è **Thi·∫øu GEMINI_API_KEY!**")
        st.info("""
        **C√°ch c·∫•u h√¨nh:**
        1. V√†o Settings ‚Üí Secrets tr√™n Streamlit Cloud
        2. Th√™m:
        ```
        GEMINI_API_KEY = "your-api-key"
        ```
        3. L·∫•y API key t·∫°i: https://makersuite.google.com/app/apikey
        """)
        st.stop()
    
    # Kh·ªüi t·∫°o session state
    if "messages" not in st.session_state:
        st.session_state.messages = []
    if "first_visit" not in st.session_state:
        st.session_state.first_visit = True

    # Header
    logo_base64 = get_base64_of_image("logo.jpg")
    st.markdown(f"""
    <div class="main-header">
        {f'<img src="data:image/jpg;base64,{logo_base64}" style="width:80px;border-radius:50%;margin-bottom:1rem;">' if logo_base64 else ''}
        <h1>ü§ñ Chatbot T∆∞ V·∫•n Tuy·ªÉn Sinh</h1>
        <h3>Tr∆∞·ªùng ƒê·∫°i h·ªçc Lu·∫≠t TP. H·ªì Ch√≠ Minh</h3>
        <p>üí¨ H·ªó tr·ª£ 24/7 | üéì T∆∞ v·∫•n chuy√™n nghi·ªáp</p>
    </div>
    """, unsafe_allow_html=True)

    # Sidebar
    with st.sidebar:
        st.markdown("### ‚öôÔ∏è C√†i ƒë·∫∑t")
        
        # Th√¥ng tin h·ªá th·ªëng
        with st.expander("üìä Tr·∫°ng th√°i h·ªá th·ªëng", expanded=False):
            st.success("‚úÖ Gemini API: ƒê√£ k·∫øt n·ªëi")
            st.info(f"üìÅ Documents: {len(get_document_files())} files")
            
            if GDRIVE_VECTORSTORE_ID:
                st.info("‚òÅÔ∏è Google Drive: ƒê√£ c·∫•u h√¨nh")
            else:
                st.warning("‚ö†Ô∏è Google Drive: Ch∆∞a c·∫•u h√¨nh")
        
        # H∆∞·ªõng d·∫´n c·∫•u h√¨nh GDrive
        with st.expander("üìñ H∆∞·ªõng d·∫´n Google Drive", expanded=False):
            st.markdown("""
            **ƒê·ªÉ s·ª≠ d·ª•ng Google Drive:**
            
            1. Upload file `vectorstore.pkl` v√† `metadata.json` l√™n GDrive
            2. Click chu·ªôt ph·∫£i ‚Üí Share ‚Üí Anyone with the link
            3. Copy File ID t·ª´ URL (ph·∫ßn sau `/d/` v√† tr∆∞·ªõc `/view`)
            4. Th√™m v√†o Secrets:
            ```
            GDRIVE_VECTORSTORE_ID = "file-id-1"
            GDRIVE_METADATA_ID = "file-id-2"
            ```
            """)
        
        # N√∫t l√†m m·ªõi
        if st.button("üîÑ L√†m m·ªõi d·ªØ li·ªáu", use_container_width=True):
            st.cache_resource.clear()
            st.rerun()
        
        st.markdown("---")
        st.markdown("""
        ### üìû Li√™n h·ªá
        **Hotline:** 1900 5555 14  
        **Email:** tuyensinh@hcmulaw.edu.vn  
        **Web:** www.hcmulaw.edu.vn
        """)

    # Kh·ªüi t·∫°o vectorstore v√† LLM
    with st.spinner("üîÑ ƒêang kh·ªüi ƒë·ªông h·ªá th·ªëng..."):
        vectorstore, stats = initialize_vectorstore()
        llm = get_gemini_llm()
        
        # Kh√¥ng d√πng chain n·ªØa, x·ª≠ l√Ω tr·ª±c ti·∫øp
        if vectorstore:
            retriever = vectorstore.as_retriever(search_kwargs={"k": 3})
        else:
            retriever = None

    # Hi·ªÉn th·ªã c√¢u h·ªèi g·ª£i √Ω n·∫øu l√† l·∫ßn ƒë·∫ßu
    if not st.session_state.messages and st.session_state.first_visit:
        display_quick_questions()
        
        st.markdown("""
        <div class="info-card">
            <h4>üí° H∆∞·ªõng d·∫´n s·ª≠ d·ª•ng:</h4>
            <ul>
                <li>üéØ Ch·ªçn c√¢u h·ªèi g·ª£i √Ω ho·∫∑c nh·∫≠p c√¢u h·ªèi c·ªßa b·∫°n</li>
                <li>üí¨ ƒê·∫∑t c√¢u h·ªèi c·ª• th·ªÉ ƒë·ªÉ ƒë∆∞·ª£c t∆∞ v·∫•n ch√≠nh x√°c</li>
                <li>üìû Li√™n h·ªá tr·ª±c ti·∫øp n·∫øu c·∫ßn h·ªó tr·ª£ kh·∫©n c·∫•p</li>
            </ul>
        </div>
        """, unsafe_allow_html=True)

    # Hi·ªÉn th·ªã l·ªãch s·ª≠ chat
    for msg in st.session_state.messages:
        with st.chat_message(msg["role"]):
            if msg["role"] == "assistant" and "category" in msg:
                st.markdown(get_category_badge(msg["category"]), unsafe_allow_html=True)
            st.markdown(msg["content"])

    # X·ª≠ l√Ω input
    prompt = None
    if hasattr(st.session_state, 'process_question'):
        prompt = st.session_state.process_question
        del st.session_state.process_question
    else:
        prompt = st.chat_input("üí¨ H√£y ƒë·∫∑t c√¢u h·ªèi...")
    if "pending_question" in st.session_state and st.session_state.pending_question:
    prompt = st.session_state.pending_question
    st.session_state.pending_question = None
    
    if prompt:
        st.session_state.first_visit = False
        
        # Hi·ªÉn th·ªã c√¢u h·ªèi
        with st.chat_message("user"):
            st.markdown(prompt)
        st.session_state.messages.append({"role": "user", "content": prompt})

        # Ph√¢n lo·∫°i v√† tr·∫£ l·ªùi
        category = classify_question(prompt)
        
        with st.chat_message("assistant"):
            st.markdown(get_category_badge(category), unsafe_allow_html=True)
            
            with st.spinner("ü§î ƒêang suy nghƒ©..."):
                try:
                    # X·ª¨ L√ù TR·ª∞C TI·∫æP kh√¥ng d√πng chain
                    if retriever:
                        # L·∫•y context t·ª´ vectorstore
                        docs = retriever.invoke(prompt)
                        context = "\n\n".join([doc.page_content for doc in docs[:3]])
                        
                        # T·∫°o prompt v·ªõi context
                        full_prompt = f"""
B·∫°n l√† chuy√™n gia t∆∞ v·∫•n c·ªßa ƒê·∫°i h·ªçc Lu·∫≠t TPHCM.

TH√îNG TIN THAM KH·∫¢O:
{context}

TH√îNG TIN LI√äN H·ªÜ:
- Hotline: 1900 5555 14 ho·∫∑c 0879 5555 14
- Email: tuyensinh@hcmulaw.edu.vn
- Website: www.hcmulaw.edu.vn

C√¢u h·ªèi: {prompt}

H√£y tr·∫£ l·ªùi d·ª±a tr√™n th√¥ng tin tham kh·∫£o ·ªü tr√™n. N·∫øu kh√¥ng c√≥ th√¥ng tin, h√£y t∆∞ v·∫•n chung v√† khuy·∫øn kh√≠ch li√™n h·ªá tr·ª±c ti·∫øp.
"""
                        answer = call_gemini_api(llm, full_prompt)

                    else:
                        # Kh√¥ng c√≥ vectorstore, d√πng API thu·∫ßn
                        answer = answer_from_external_api(prompt, llm, category)
                    
                    st.markdown(answer)
                    
                except Exception as e:
                    answer = f"""
‚ùå **L·ªói h·ªá th·ªëng**

Vui l√≤ng li√™n h·ªá:
üìû Hotline: 1900 5555 14 ho·∫∑c 0879 5555 14
üìß Email: tuyensinh@hcmulaw.edu.vn

_(L·ªói: {str(e)[:100]})_
"""
                    st.error(answer)

            st.session_state.messages.append({
                "role": "assistant",
                "content": answer,
                "category": category
            })
            st.rerun()

    # Footer
    st.markdown("---")
    st.markdown("""
    <div class="footer">
        <h4>üèõÔ∏è Tr∆∞·ªùng ƒê·∫°i h·ªçc Lu·∫≠t TP. H·ªì Ch√≠ Minh</h4>
        <p>üìç 2 Nguy·ªÖn T·∫•t Th√†nh, Ph∆∞·ªùng 12, Qu·∫≠n 4, TP.HCM</p>
        <p>üìû Hotline: 1900 5555 14 | Email: tuyensinh@hcmulaw.edu.vn</p>
        <p>üåê www.hcmulaw.edu.vn | üìò facebook.com/hcmulaw</p>
        <p style="margin-top:1rem;opacity:0.8;font-size:0.85em;">
            ü§ñ Chatbot v2.1 | Ph√°t tri·ªÉn b·ªüi Lvphung - CNTT
        </p>
    </div>
    """, unsafe_allow_html=True)

if __name__ == "__main__":
    main()
