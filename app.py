import streamlit as st
from langchain_community.document_loaders import PyPDFLoader, Docx2txtLoader, TextLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings
from langchain.chains import ConversationalRetrievalChain
from langchain_google_genai import GoogleGenerativeAI
from langchain.memory import ConversationBufferWindowMemory
from langchain.prompts import PromptTemplate
import os
import pickle
import json
from datetime import datetime
import gdown
import tempfile
import glob
from pathlib import Path
from dotenv import load_dotenv
import base64

def get_base64_of_image(path):
    """Convert image to base64 string"""
    try:
        with open(path, "rb") as img_file:
            return base64.b64encode(img_file.read()).decode()
    except Exception as e:
        return ""

# Cáº¥u hÃ¬nh trang
st.set_page_config(
    page_title="Chatbot TÆ° Váº¥n - Äáº¡i há»c Luáº­t TPHCM",
    page_icon="âš–ï¸",
    layout="wide",
    initial_sidebar_state="expanded"
)

# CSS tá»‘i Æ°u hÆ¡n
st.markdown("""
<style>
    @import url('https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap');
    
    * { font-family: 'Inter', sans-serif; }
    
    .main-header {
        background: linear-gradient(135deg, #1e3c72 0%, #2a5298 100%);
        padding: 2rem;
        border-radius: 20px;
        color: white;
        text-align: center;
        margin-bottom: 2rem;
        box-shadow: 0 10px 30px rgba(30, 60, 114, 0.3);
    }
    
    .main-header h1 {
        font-size: 2.2rem;
        font-weight: 700;
        margin: 0.5rem 0;
    }
    
    .chat-message {
        padding: 1.5rem;
        border-radius: 15px;
        margin: 1rem 0;
        box-shadow: 0 5px 15px rgba(0,0,0,0.1);
    }
    
    .user-message {
        background: linear-gradient(135deg, #e3f2fd 0%, #bbdefb 100%);
        border-left: 5px solid #2196f3;
    }
    
    .assistant-message {
        background: linear-gradient(135deg, #f3e5f5 0%, #e1bee7 100%);
        border-left: 5px solid #9c27b0;
    }
    
    .category-badge {
        display: inline-block;
        padding: 0.4rem 0.8rem;
        border-radius: 15px;
        font-size: 0.75rem;
        font-weight: 600;
        margin-bottom: 0.5rem;
    }
    
    .badge-tuyensinh { background: #e3f2fd; color: #1565c0; }
    .badge-hocphi { background: #e8f5e8; color: #2e7d32; }
    .badge-chuongtrinh { background: #f3e5f5; color: #6a1b9a; }
    
    .info-card {
        background: linear-gradient(135deg, #fff3e0 0%, #ffe0b2 100%);
        border: 1px solid #ffb74d;
        border-radius: 15px;
        padding: 1.5rem;
        margin: 1rem 0;
    }
    
    .footer {
        background: linear-gradient(135deg, #1e3c72 0%, #2a5298 100%);
        color: white;
        padding: 2rem;
        border-radius: 15px;
        margin-top: 3rem;
        text-align: center;
    }
    
    .stButton>button {
        background: linear-gradient(135deg, #2196f3 0%, #1976d2 100%);
        color: white;
        border: none;
        border-radius: 10px;
        padding: 0.5rem 1rem;
        font-weight: 500;
        transition: all 0.3s ease;
    }
    
    .stButton>button:hover {
        background: linear-gradient(135deg, #1976d2 0%, #1565c0 100%);
        transform: translateY(-2px);
    }
</style>
""", unsafe_allow_html=True)

# Load biáº¿n mÃ´i trÆ°á»ng
load_dotenv()
gemini_api_key = st.secrets.get("GEMINI_API_KEY") or os.getenv("GEMINI_API_KEY")
GDRIVE_VECTORSTORE_ID = st.secrets.get("GDRIVE_VECTORSTORE_ID") or os.getenv("GDRIVE_VECTORSTORE_ID")
GDRIVE_METADATA_ID = st.secrets.get("GDRIVE_METADATA_ID") or os.getenv("GDRIVE_METADATA_ID")

# Cáº¥u hÃ¬nh Ä‘Æ°á»ng dáº«n
DOCUMENTS_PATH = "documents"
VECTORSTORE_PATH = "vectorstore"

for path in [DOCUMENTS_PATH, VECTORSTORE_PATH]:
    Path(path).mkdir(exist_ok=True)

# Template prompt
COUNSELING_PROMPT_TEMPLATE = """
Báº¡n lÃ  chuyÃªn gia tÆ° váº¥n tuyá»ƒn sinh TrÆ°á»ng Äáº¡i há»c Luáº­t ThÃ nh phá»‘ Há»“ ChÃ­ Minh.

THÃ”NG TIN LIÃŠN Há»† CHÃNH THá»¨C:
- Hotline tuyá»ƒn sinh: 1900 5555 14 hoáº·c 0879 5555 14
- Email: tuyensinh@hcmulaw.edu.vn
- Äiá»‡n thoáº¡i: (028) 39400 989
- Äá»‹a chá»‰: 2 Nguyá»…n Táº¥t ThÃ nh, PhÆ°á»ng 12, Quáº­n 4, TP.HCM
- Website: www.hcmulaw.edu.vn

NguyÃªn táº¯c tráº£ lá»i:
1. ThÃ¢n thiá»‡n, chuyÃªn nghiá»‡p
2. Cung cáº¥p thÃ´ng tin chÃ­nh xÃ¡c vá» Äáº¡i há»c Luáº­t TPHCM
3. KHÃ”NG sá»­ dá»¥ng placeholder, luÃ´n dÃ¹ng thÃ´ng tin liÃªn há»‡ cá»¥ thá»ƒ á»Ÿ trÃªn
4. Náº¿u khÃ´ng cháº¯c cháº¯n, khuyáº¿n khÃ­ch liÃªn há»‡ trá»±c tiáº¿p

ThÃ´ng tin tham kháº£o: {context}
Lá»‹ch sá»­ há»™i thoáº¡i: {chat_history}
CÃ¢u há»i: {question}

Tráº£ lá»i (tiáº¿ng Viá»‡t):
"""

@st.cache_resource
def load_embeddings():
    return HuggingFaceEmbeddings(
        model_name="keepitreal/vietnamese-sbert",
        model_kwargs={'device': 'cpu'},
        encode_kwargs={'normalize_embeddings': True}
    )

embeddings = load_embeddings()

def download_from_gdrive(file_id, output_path):
    """Download file tá»« Google Drive"""
    try:
        url = f'https://drive.google.com/uc?id={file_id}'
        gdown.download(url, output_path, quiet=True)
        return True
    except Exception as e:
        st.warning(f"âš ï¸ KhÃ´ng thá»ƒ táº£i tá»« GDrive: {e}")
        return False

def get_document_files():
    """Láº¥y danh sÃ¡ch file trong documents"""
    files = []
    for ext in ['*.pdf', '*.docx', '*.txt']:
        files.extend(glob.glob(os.path.join(DOCUMENTS_PATH, '**', ext), recursive=True))
    return files

def get_file_hash(file_path):
    """Táº¡o hash cho file"""
    stat = os.stat(file_path)
    return f"{stat.st_mtime}_{stat.st_size}"

def load_cached_vectorstore():
    """Load vector store tá»« Google Drive"""
    if not GDRIVE_VECTORSTORE_ID or not GDRIVE_METADATA_ID:
        return None, {}
    
    temp_dir = tempfile.mkdtemp()
    vectorstore_path = os.path.join(temp_dir, "vectorstore.pkl")
    metadata_path = os.path.join(temp_dir, "metadata.json")
    
    try:
        if not download_from_gdrive(GDRIVE_VECTORSTORE_ID, vectorstore_path):
            return None, {}
        if not download_from_gdrive(GDRIVE_METADATA_ID, metadata_path):
            return None, {}
        
        with open(vectorstore_path, 'rb') as f:
            vectorstore = pickle.load(f)
        
        with open(metadata_path, 'r', encoding='utf-8') as f:
            metadata = json.load(f)
        
        # Cleanup
        os.remove(vectorstore_path)
        os.remove(metadata_path)
        os.rmdir(temp_dir)
        
        return vectorstore, metadata
        
    except Exception as e:
        st.error(f"âŒ Lá»—i load vectorstore: {e}")
        return None, {}

def process_documents(file_paths):
    """Xá»­ lÃ½ documents"""
    documents = []
    processed = []
    failed = []
    
    for file_path in file_paths:
        try:
            ext = Path(file_path).suffix.lower()
            
            if ext == ".pdf":
                loader = PyPDFLoader(file_path)
            elif ext == ".docx":
                loader = Docx2txtLoader(file_path)
            elif ext == ".txt":
                loader = TextLoader(file_path, encoding='utf-8')
            else:
                failed.append(f"{file_path} (khÃ´ng há»— trá»£)")
                continue
            
            docs = loader.load()
            for doc in docs:
                doc.metadata['source_file'] = os.path.basename(file_path)
                doc.metadata['processed_time'] = datetime.now().isoformat()
            
            documents.extend(docs)
            processed.append(file_path)
            
        except Exception as e:
            failed.append(f"{file_path} ({str(e)})")
    
    return documents, processed, failed

def create_vector_store(documents):
    """Táº¡o vector store"""
    if not documents:
        return None
    
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=200,
        separators=['\n\n', '\n', '.', '!', '?', ';', ':', ' ']
    )
    texts = text_splitter.split_documents(documents)
    texts = [t for t in texts if len(t.page_content.strip()) > 50]
    
    if not texts:
        return None
    
    return FAISS.from_documents(texts, embeddings)

@st.cache_resource
def initialize_vectorstore():
    """Khá»Ÿi táº¡o vectorstore"""
    # Thá»­ load tá»« cache trÆ°á»›c
    vectorstore, metadata = load_cached_vectorstore()
    if vectorstore:
        return vectorstore, metadata.get('stats', {})
    
    # Náº¿u khÃ´ng cÃ³ cache, xá»­ lÃ½ local files
    current_files = get_document_files()
    if not current_files:
        st.warning("âš ï¸ KhÃ´ng tÃ¬m tháº¥y file nÃ o")
        return None, {}
    
    with st.spinner("ğŸ”„ Äang xá»­ lÃ½ tÃ i liá»‡u..."):
        documents, processed, failed = process_documents(current_files)
        
        if not documents:
            st.error("âŒ KhÃ´ng thá»ƒ xá»­ lÃ½ file")
            return None, {}
        
        vectorstore = create_vector_store(documents)
        
        if vectorstore:
            stats = {
                'total_files': len(current_files),
                'processed_files': len(processed),
                'failed_files': len(failed),
                'total_chunks': vectorstore.index.ntotal,
                'last_updated': datetime.now().isoformat()
            }
            return vectorstore, stats
    
    return None, {}

def classify_question(question):
    """PhÃ¢n loáº¡i cÃ¢u há»i"""
    question_lower = question.lower()
    
    categories = {
        "Tuyá»ƒn sinh": ["tuyá»ƒn sinh", "Ä‘Äƒng kÃ½", "há»“ sÆ¡", "Ä‘iá»ƒm chuáº©n", "xÃ©t tuyá»ƒn"],
        "Há»c phÃ­": ["há»c phÃ­", "chi phÃ­", "miá»…n giáº£m", "há»c bá»•ng"],
        "ChÆ°Æ¡ng trÃ¬nh Ä‘Ã o táº¡o": ["chÆ°Æ¡ng trÃ¬nh", "mÃ´n há»c", "tÃ­n chá»‰", "ngÃ nh"],
    }
    
    for category, keywords in categories.items():
        if any(kw in question_lower for kw in keywords):
            return category
    return "KhÃ¡c"

def get_category_badge(category):
    """Táº¡o badge cho category"""
    badge_map = {
        "Tuyá»ƒn sinh": "badge-tuyensinh",
        "Há»c phÃ­": "badge-hocphi",
        "ChÆ°Æ¡ng trÃ¬nh Ä‘Ã o táº¡o": "badge-chuongtrinh"
    }
    badge_class = badge_map.get(category, "badge-tuyensinh")
    return f'<span class="category-badge {badge_class}">{category}</span>'

def create_conversational_chain(vector_store, llm):
    """Táº¡o chain"""
    prompt = PromptTemplate(
        template=COUNSELING_PROMPT_TEMPLATE,
        input_variables=["context", "chat_history", "question"]
    )
    
    memory = ConversationBufferWindowMemory(
        k=5,
        memory_key="chat_history",
        return_messages=True,
        output_key="answer"
    )
    
    return ConversationalRetrievalChain.from_llm(
        llm=llm,
        retriever=vector_store.as_retriever(search_kwargs={"k": 5}),
        memory=memory,
        return_source_documents=True,
        combine_docs_chain_kwargs={"prompt": prompt}
    )

@st.cache_resource
def get_gemini_llm():
    """Khá»Ÿi táº¡o Gemini LLM vá»›i cáº¥u hÃ¬nh tá»‘i Æ°u"""
    try:
        return GoogleGenerativeAI(
            model="gemini-1.5-flash",  # Hoáº·c thá»­ "models/gemini-1.5-flash"
            google_api_key=gemini_api_key,
            temperature=0.3,
            max_output_tokens=2000
        )
    except Exception as e:
        st.error(f"âŒ Lá»—i khá»Ÿi táº¡o Gemini: {e}")
        # Fallback sang model khÃ¡c náº¿u cáº§n
        return GoogleGenerativeAI(
            model="gemini-pro",
            google_api_key=gemini_api_key,
            temperature=0.3
        )

def answer_from_external_api(prompt, llm, question_category):
    """Tráº£ lá»i tá»« API"""
    enhanced_prompt = f"""
Báº¡n lÃ  chuyÃªn gia tÆ° váº¥n {question_category.lower()} cá»§a Äáº¡i há»c Luáº­t TPHCM.

THÃ”NG TIN LIÃŠN Há»† (Báº®T BUá»˜C Sá»¬ Dá»¤NG):
- Hotline: 1900 5555 14 hoáº·c 0879 5555 14
- Email: tuyensinh@hcmulaw.edu.vn
- Äiá»‡n thoáº¡i: (028) 39400 989
- Äá»‹a chá»‰: 2 Nguyá»…n Táº¥t ThÃ nh, PhÆ°á»ng 12, Quáº­n 4, TP.HCM
- Website: www.hcmulaw.edu.vn

CÃ¢u há»i: {prompt}

QUY Táº®C:
- KHÃ”NG dÃ¹ng placeholder nhÆ° [Sá»‘ Ä‘iá»‡n thoáº¡i], [Email]
- LuÃ´n dÃ¹ng thÃ´ng tin cá»¥ thá»ƒ á»Ÿ trÃªn
- Káº¿t thÃºc báº±ng thÃ´ng tin liÃªn há»‡ náº¿u cáº§n

Tráº£ lá»i thÃ¢n thiá»‡n, chuyÃªn nghiá»‡p:
"""
    
    try:
        response = llm.invoke(enhanced_prompt)
        
        # Thay tháº¿ placeholder cÃ²n sÃ³t
        replacements = {
            "[Sá»‘ Ä‘iá»‡n thoáº¡i": "1900 5555 14 hoáº·c 0879 5555 14",
            "[Email": "tuyensinh@hcmulaw.edu.vn",
            "[Website": "www.hcmulaw.edu.vn"
        }
        
        for placeholder, actual in replacements.items():
            if placeholder in response:
                response = response.replace(placeholder + "]", actual)
        
        return response
        
    except Exception as e:
        return f"""
Xin lá»—i, há»‡ thá»‘ng gáº·p sá»± cá»‘. Vui lÃ²ng liÃªn há»‡:

ğŸ“ Hotline: 1900 5555 14 hoáº·c 0879 5555 14
ğŸ“§ Email: tuyensinh@hcmulaw.edu.vn
ğŸŒ Website: www.hcmulaw.edu.vn
ğŸ“ Äá»‹a chá»‰: 2 Nguyá»…n Táº¥t ThÃ nh, P.12, Q.4, TP.HCM

Lá»—i: {str(e)}
"""

def display_quick_questions():
    """Hiá»ƒn thá»‹ cÃ¢u há»i gá»£i Ã½"""
    st.markdown("### ğŸ’¡ CÃ¢u há»i thÆ°á»ng gáº·p")
    
    questions = [
        "ğŸ“ Thá»§ tá»¥c Ä‘Äƒng kÃ½ xÃ©t tuyá»ƒn?",
        "ğŸ’° Há»c phÃ­ cá»§a trÆ°á»ng?",
        "ğŸ“š CÃ¡c ngÃ nh há»c?",
        "ğŸ  TrÆ°á»ng cÃ³ kÃ½ tÃºc xÃ¡ khÃ´ng?",
        "ğŸ“ CÆ¡ há»™i viá»‡c lÃ m?",
        "ğŸ“ ThÃ´ng tin liÃªn há»‡?"
    ]
    
    cols = st.columns(2)
    for i, q in enumerate(questions):
        with cols[i % 2]:
            if st.button(q, key=f"q_{i}", use_container_width=True):
                clean_q = q.split(" ", 1)[1]
                st.session_state.messages.append({"role": "user", "content": clean_q})
                st.session_state.process_question = clean_q
                st.rerun()

def main():
    # Kiá»ƒm tra API key
    if not gemini_api_key:
        st.error("âš ï¸ Thiáº¿u GEMINI_API_KEY! Vui lÃ²ng cáº¥u hÃ¬nh trong Streamlit Secrets")
        st.stop()
    
    # Khá»Ÿi táº¡o session state
    if "messages" not in st.session_state:
        st.session_state.messages = []
    if "first_visit" not in st.session_state:
        st.session_state.first_visit = True

    # Header
    logo_base64 = get_base64_of_image("logo.jpg")
    st.markdown(f"""
    <div class="main-header">
        {f'<img src="data:image/jpg;base64,{logo_base64}" style="width:80px;border-radius:50%;margin-bottom:1rem;">' if logo_base64 else ''}
        <h1>ğŸ¤– Chatbot TÆ° Váº¥n Tuyá»ƒn Sinh</h1>
        <h3>TrÆ°á»ng Äáº¡i há»c Luáº­t TP. Há»“ ChÃ­ Minh</h3>
        <p>ğŸ’¬ Há»— trá»£ 24/7 | ğŸ“ TÆ° váº¥n chuyÃªn nghiá»‡p</p>
    </div>
    """, unsafe_allow_html=True)

    # Sidebar
    with st.sidebar:
        st.markdown("### âš™ï¸ CÃ i Ä‘áº·t")
        
        # ThÃ´ng tin há»‡ thá»‘ng
        with st.expander("ğŸ“Š ThÃ´ng tin há»‡ thá»‘ng", expanded=False):
            st.info(f"""
            **Tráº¡ng thÃ¡i:**
            - âœ… Gemini API: {'ÄÃ£ káº¿t ná»‘i' if gemini_api_key else 'âŒ ChÆ°a cáº¥u hÃ¬nh'}
            - ğŸ“ Documents: {len(get_document_files())} files
            """)
        
        # NÃºt lÃ m má»›i
        if st.button("ğŸ”„ LÃ m má»›i dá»¯ liá»‡u", use_container_width=True):
            st.cache_resource.clear()
            st.rerun()
        
        st.markdown("---")
        st.markdown("""
        ### ğŸ“ LiÃªn há»‡
        **Hotline:** 1900 5555 14  
        **Email:** tuyensinh@hcmulaw.edu.vn  
        **Web:** www.hcmulaw.edu.vn
        """)

    # Khá»Ÿi táº¡o vectorstore
    with st.spinner("ğŸ”„ Äang khá»Ÿi táº¡o há»‡ thá»‘ng..."):
        vectorstore, stats = initialize_vectorstore()
        llm = get_gemini_llm()
        chain = create_conversational_chain(vectorstore, llm) if vectorstore else None

    # Hiá»ƒn thá»‹ cÃ¢u há»i gá»£i Ã½ náº¿u lÃ  láº§n Ä‘áº§u
    if not st.session_state.messages and st.session_state.first_visit:
        display_quick_questions()
        
        st.markdown("""
        <div class="info-card">
            <h4>ğŸ’¡ HÆ°á»›ng dáº«n sá»­ dá»¥ng:</h4>
            <ul>
                <li>ğŸ¯ Chá»n cÃ¢u há»i gá»£i Ã½ hoáº·c nháº­p cÃ¢u há»i cá»§a báº¡n</li>
                <li>ğŸ’¬ Äáº·t cÃ¢u há»i cá»¥ thá»ƒ Ä‘á»ƒ Ä‘Æ°á»£c tÆ° váº¥n chÃ­nh xÃ¡c</li>
                <li>ğŸ“ LiÃªn há»‡ trá»±c tiáº¿p náº¿u cáº§n há»— trá»£ kháº©n cáº¥p</li>
            </ul>
        </div>
        """, unsafe_allow_html=True)

    # Hiá»ƒn thá»‹ lá»‹ch sá»­ chat
    for msg in st.session_state.messages:
        with st.chat_message(msg["role"]):
            if msg["role"] == "assistant" and "category" in msg:
                st.markdown(get_category_badge(msg["category"]), unsafe_allow_html=True)
            st.markdown(msg["content"])

    # Xá»­ lÃ½ input
    prompt = None
    if hasattr(st.session_state, 'process_question'):
        prompt = st.session_state.process_question
        del st.session_state.process_question
    else:
        prompt = st.chat_input("ğŸ’¬ HÃ£y Ä‘áº·t cÃ¢u há»i...")

    if prompt:
        st.session_state.first_visit = False
        
        # Hiá»ƒn thá»‹ cÃ¢u há»i
        with st.chat_message("user"):
            st.markdown(prompt)
        st.session_state.messages.append({"role": "user", "content": prompt})

        # PhÃ¢n loáº¡i vÃ  tráº£ lá»i
        category = classify_question(prompt)
        
        with st.chat_message("assistant"):
            st.markdown(get_category_badge(category), unsafe_allow_html=True)
            
            with st.spinner("ğŸ¤” Äang suy nghÄ©..."):
                try:
                    if chain:
                        response = chain({"question": prompt})
                        answer = response["answer"]
                    else:
                        answer = answer_from_external_api(prompt, llm, category)
                    
                    st.markdown(answer)
                    
                except Exception as e:
                    answer = f"""
âŒ **Lá»—i há»‡ thá»‘ng**

Vui lÃ²ng liÃªn há»‡:
ğŸ“ Hotline: 1900 5555 14 hoáº·c 0879 5555 14
ğŸ“§ Email: tuyensinh@hcmulaw.edu.vn

Lá»—i: {str(e)}
"""
                    st.error(answer)

            st.session_state.messages.append({
                "role": "assistant",
                "content": answer,
                "category": category
            })

    # Footer
    st.markdown("---")
    st.markdown("""
    <div class="footer">
        <h4>ğŸ›ï¸ TrÆ°á»ng Äáº¡i há»c Luáº­t TP. Há»“ ChÃ­ Minh</h4>
        <p>ğŸ“ 2 Nguyá»…n Táº¥t ThÃ nh, PhÆ°á»ng 12, Quáº­n 4, TP.HCM</p>
        <p>ğŸ“ Hotline: 1900 5555 14 | Email: tuyensinh@hcmulaw.edu.vn</p>
        <p>ğŸŒ www.hcmulaw.edu.vn | ğŸ“˜ facebook.com/hcmulaw</p>
        <p style="margin-top:1rem;opacity:0.8;font-size:0.85em;">
            ğŸ¤– Chatbot v2.0 | PhÃ¡t triá»ƒn bá»Ÿi Lvphung - CNTT
        </p>
    </div>
    """, unsafe_allow_html=True)

if __name__ == "__main__":
    main()
