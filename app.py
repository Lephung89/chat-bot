import streamlit as st
from langchain_community.document_loaders import PyPDFLoader, Docx2txtLoader, TextLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings
from langchain.chains import ConversationalRetrievalChain
import requests
import json
from langchain.memory import ConversationBufferWindowMemory
from langchain.prompts import PromptTemplate
import os
import pickle
import json
from datetime import datetime
import requests
import tempfile
import glob
from pathlib import Path
from dotenv import load_dotenv
import base64
import warnings, logging
warnings.filterwarnings("ignore")
logging.getLogger().setLevel(logging.ERROR)
debug = False
verbose = False


def get_base64_of_image(path):
    """Convert image to base64 string"""
    try:
        with open(path, "rb") as img_file:
            return base64.b64encode(img_file.read()).decode()
    except Exception as e:
        return ""

# Cáº¥u hÃ¬nh trang
st.set_page_config(
    page_title="Chatbot TÆ° Váº¥n - Äáº¡i há»c Luáº­t TPHCM",
    page_icon="âš–ï¸",
    layout="wide",
    initial_sidebar_state="expanded"
)

# CSS tá»‘i Æ°u
st.markdown("""
<style>
.header-container {
    background: linear-gradient(90deg, #003366, #004c99);
    color: white;
    padding: 24px 0;
    border-radius: 16px;
    text-align: center;
    box-shadow: 0 4px 15px rgba(0,0,0,0.25);
    margin-bottom: 20px;
}
.header-container h1 {
    font-size: 2rem;
    margin-bottom: 0.4rem;
    font-weight: 700;
}
.header-container h3 {
    font-size: 1.2rem;
    font-weight: 400;
    margin-top: 0;
    opacity: 0.9;
}
.header-container p {
    font-size: 1rem;
    margin-top: 0.2rem;
    opacity: 0.85;
}
</style>

<div class="header-container">
    <h1>ğŸ¤– Chatbot TÆ° Váº¥n Tuyá»ƒn Sinh</h1>
    <h3>TrÆ°á»ng Äáº¡i há»c Luáº­t TP. Há»“ ChÃ­ Minh</h3>
    <p>ğŸ’¬ Há»— trá»£ 24/7 &nbsp; | &nbsp; ğŸ“ TÆ° váº¥n chuyÃªn nghiá»‡p</p>
</div>
""", unsafe_allow_html=True)


# Load biáº¿n mÃ´i trÆ°á»ng
load_dotenv()

# Äá»ŒC API KEY Tá»ª SECRETS TRÆ¯á»šC, SAU ÄÃ“ Má»šI Äáº¾N .env
try:
    gemini_api_key = st.secrets["GEMINI_API_KEY"]
except:
    gemini_api_key = os.getenv("GEMINI_API_KEY")

try:
    GDRIVE_VECTORSTORE_ID = st.secrets["GDRIVE_VECTORSTORE_ID"]
except:
    GDRIVE_VECTORSTORE_ID = os.getenv("GDRIVE_VECTORSTORE_ID")

try:
    GDRIVE_METADATA_ID = st.secrets["GDRIVE_METADATA_ID"]
except:
    GDRIVE_METADATA_ID = os.getenv("GDRIVE_METADATA_ID")

# Cáº¥u hÃ¬nh Ä‘Æ°á»ng dáº«n
DOCUMENTS_PATH = "documents"
VECTORSTORE_PATH = "vectorstore"

for path in [DOCUMENTS_PATH, VECTORSTORE_PATH]:
    Path(path).mkdir(exist_ok=True)

# Template prompt
COUNSELING_PROMPT_TEMPLATE = """
Báº¡n lÃ  chuyÃªn gia tÆ° váº¥n tuyá»ƒn sinh TrÆ°á»ng Äáº¡i há»c Luáº­t ThÃ nh phá»‘ Há»“ ChÃ­ Minh.

THÃ”NG TIN LIÃŠN Há»† CHÃNH THá»¨C:
- Hotline tuyá»ƒn sinh: 1900 5555 14 hoáº·c 0879 5555 14
- Email: tuyensinh@hcmulaw.edu.vn
- Äiá»‡n thoáº¡i: (028) 39400 989
- Äá»‹a chá»‰: 2 Nguyá»…n Táº¥t ThÃ nh, PhÆ°á»ng 12, Quáº­n 4, TP.HCM
- Website: www.hcmulaw.edu.vn

NguyÃªn táº¯c tráº£ lá»i:
1. ThÃ¢n thiá»‡n, chuyÃªn nghiá»‡p
2. Cung cáº¥p thÃ´ng tin chÃ­nh xÃ¡c vá» Äáº¡i há»c Luáº­t TPHCM
3. KHÃ”NG sá»­ dá»¥ng placeholder, luÃ´n dÃ¹ng thÃ´ng tin liÃªn há»‡ cá»¥ thá»ƒ á»Ÿ trÃªn
4. Náº¿u khÃ´ng cháº¯c cháº¯n, khuyáº¿n khÃ­ch liÃªn há»‡ trá»±c tiáº¿p

ThÃ´ng tin tham kháº£o: {context}
Lá»‹ch sá»­ há»™i thoáº¡i: {chat_history}
CÃ¢u há»i: {question}

Tráº£ lá»i (tiáº¿ng Viá»‡t):
"""

@st.cache_resource
def load_embeddings():
    return HuggingFaceEmbeddings(
        model_name="keepitreal/vietnamese-sbert",
        model_kwargs={'device': 'cpu'},
        encode_kwargs={'normalize_embeddings': True}
    )

embeddings = load_embeddings()

def download_from_gdrive_direct(file_id, output_path):
    """
    Download file tá»« Google Drive báº±ng requests (khÃ´ng dÃ¹ng gdown)
    File pháº£i Ä‘Æ°á»£c share cÃ´ng khai: Anyone with the link
    """
    try:
        # URL download trá»±c tiáº¿p tá»« GDrive
        url = f"https://drive.google.com/uc?export=download&id={file_id}"
        
        session = requests.Session()
        response = session.get(url, stream=True)
        
        # Xá»­ lÃ½ virus scan warning cá»§a GDrive (file lá»›n)
        for key, value in response.cookies.items():
            if key.startswith('download_warning'):
                params = {'export': 'download', 'id': file_id, 'confirm': value}
                response = session.get(url, params=params, stream=True)
                break
        
        # LÆ°u file
        with open(output_path, 'wb') as f:
            for chunk in response.iter_content(chunk_size=32768):
                if chunk:
                    f.write(chunk)
        
        return True
        
    except Exception as e:
        #st.warning(f"âš ï¸ KhÃ´ng thá»ƒ táº£i tá»« GDrive: {e}")
        return False

def get_document_files():
    """Láº¥y danh sÃ¡ch file trong documents"""
    files = []
    for ext in ['*.pdf', '*.docx', '*.txt']:
        files.extend(glob.glob(os.path.join(DOCUMENTS_PATH, '**', ext), recursive=True))
    return files

def get_file_hash(file_path):
    """Táº¡o hash cho file"""
    stat = os.stat(file_path)
    return f"{stat.st_mtime}_{stat.st_size}"

def load_cached_vectorstore():
    """Load vector store tá»« Google Drive"""
    if not GDRIVE_VECTORSTORE_ID or not GDRIVE_METADATA_ID:
        #st.info("â„¹ï¸ ChÆ°a cáº¥u hÃ¬nh Google Drive, sá»­ dá»¥ng xá»­ lÃ½ local")
        return None, {}
    
    temp_dir = tempfile.mkdtemp()
    vectorstore_path = os.path.join(temp_dir, "vectorstore.pkl")
    metadata_path = os.path.join(temp_dir, "metadata.json")
    
    try:
        # Download báº±ng requests thay vÃ¬ gdown
        if not download_from_gdrive_direct(GDRIVE_VECTORSTORE_ID, vectorstore_path):
            return None, {}
        if not download_from_gdrive_direct(GDRIVE_METADATA_ID, metadata_path):
            return None, {}
        
        with open(vectorstore_path, 'rb') as f:
            vectorstore = pickle.load(f)
        
        with open(metadata_path, 'r', encoding='utf-8') as f:
            metadata = json.load(f)
        
        # Cleanup
        os.remove(vectorstore_path)
        os.remove(metadata_path)
        os.rmdir(temp_dir)
        
        #st.success("âœ… ÄÃ£ load vectorstore tá»« Google Drive")
        return vectorstore, metadata
        
    except Exception as e:
        st.warning(f"âš ï¸ KhÃ´ng thá»ƒ load tá»« GDrive: {e}")
        # Cleanup náº¿u cÃ³ lá»—i
        try:
            if os.path.exists(vectorstore_path):
                os.remove(vectorstore_path)
            if os.path.exists(metadata_path):
                os.remove(metadata_path)
            if os.path.exists(temp_dir):
                os.rmdir(temp_dir)
        except:
            pass
        return None, {}

def process_documents(file_paths):
    """Xá»­ lÃ½ documents"""
    documents = []
    processed = []
    failed = []
    
    for file_path in file_paths:
        try:
            ext = Path(file_path).suffix.lower()
            
            if ext == ".pdf":
                loader = PyPDFLoader(file_path)
            elif ext == ".docx":
                loader = Docx2txtLoader(file_path)
            elif ext == ".txt":
                loader = TextLoader(file_path, encoding='utf-8')
            else:
                failed.append(f"{file_path} (khÃ´ng há»— trá»£)")
                continue
            
            docs = loader.load()
            for doc in docs:
                doc.metadata['source_file'] = os.path.basename(file_path)
                doc.metadata['processed_time'] = datetime.now().isoformat()
            
            documents.extend(docs)
            processed.append(file_path)
            
        except Exception as e:
            failed.append(f"{file_path} ({str(e)})")
    
    return documents, processed, failed

def create_vector_store(documents):
    """Táº¡o vector store"""
    if not documents:
        return None
    
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=200,
        separators=['\n\n', '\n', '.', '!', '?', ';', ':', ' ']
    )
    texts = text_splitter.split_documents(documents)
    texts = [t for t in texts if len(t.page_content.strip()) > 50]
    
    if not texts:
        return None
    
    return FAISS.from_documents(texts, embeddings)

@st.cache_resource
def initialize_vectorstore():
    """Khá»Ÿi táº¡o vectorstore"""
    # Thá»­ load tá»« GDrive trÆ°á»›c
    vectorstore, metadata = load_cached_vectorstore()
    if vectorstore:
        return vectorstore, metadata.get('stats', {})
    
    # Náº¿u khÃ´ng cÃ³ GDrive, xá»­ lÃ½ local files
    st.info("â„¹ï¸ Äang xá»­ lÃ½ tÃ i liá»‡u local...")
    current_files = get_document_files()
    
    if not current_files:
        st.warning("âš ï¸ KhÃ´ng tÃ¬m tháº¥y file nÃ o trong thÆ° má»¥c documents")
        return None, {}
    
    with st.spinner("ğŸ”„ Äang xá»­ lÃ½ tÃ i liá»‡u..."):
        documents, processed, failed = process_documents(current_files)
        
        if not documents:
            st.error("âŒ KhÃ´ng thá»ƒ xá»­ lÃ½ file nÃ o")
            return None, {}
        
        vectorstore = create_vector_store(documents)
        
        if vectorstore:
            stats = {
                'total_files': len(current_files),
                'processed_files': len(processed),
                'failed_files': len(failed),
                'total_chunks': vectorstore.index.ntotal,
                'last_updated': datetime.now().isoformat()
            }
            st.success(f"âœ… ÄÃ£ xá»­ lÃ½ {len(processed)} files thÃ nh cÃ´ng!")
            return vectorstore, stats
    
    return None, {}

def classify_question(question):
    """PhÃ¢n loáº¡i cÃ¢u há»i"""
    question_lower = question.lower()
    
    categories = {
        "Tuyá»ƒn sinh": ["tuyá»ƒn sinh", "Ä‘Äƒng kÃ½", "há»“ sÆ¡", "Ä‘iá»ƒm chuáº©n", "xÃ©t tuyá»ƒn"],
        "Há»c phÃ­": ["há»c phÃ­", "chi phÃ­", "miá»…n giáº£m", "há»c bá»•ng"],
        "ChÆ°Æ¡ng trÃ¬nh Ä‘Ã o táº¡o": ["chÆ°Æ¡ng trÃ¬nh", "mÃ´n há»c", "tÃ­n chá»‰", "ngÃ nh"],
    }
    
    for category, keywords in categories.items():
        if any(kw in question_lower for kw in keywords):
            return category
    return "KhÃ¡c"

def get_category_badge(category):
    """Táº¡o badge cho category"""
    badge_map = {
        "Tuyá»ƒn sinh": "badge-tuyensinh",
        "Há»c phÃ­": "badge-hocphi",
        "ChÆ°Æ¡ng trÃ¬nh Ä‘Ã o táº¡o": "badge-chuongtrinh"
    }
    badge_class = badge_map.get(category, "badge-tuyensinh")
    return f'<span class="category-badge {badge_class}">{category}</span>'

def create_conversational_chain(vector_store, llm):
    """
    Táº¡o chain vá»›i Google Generative AI
    KhÃ´ng dÃ¹ng LangChain chain ná»¯a - xá»­ lÃ½ trá»±c tiáº¿p
    """
    # Tráº£ vá» tuple: (vectorstore, llm) Ä‘á»ƒ xá»­ lÃ½ manual
    return (vector_store, llm)

@st.cache_resource
def get_gemini_llm():
    """
    Táº¡o Gemini client dÃ¹ng REST API trá»±c tiáº¿p
    GIáº¢I PHÃP CUá»I CÃ™NG - LUÃ”N HOáº T Äá»˜NG
    """
    if not gemini_api_key:
        st.error("âŒ Thiáº¿u GEMINI_API_KEY!")
        st.stop()
    
    # Test API key
    test_url = f"https://generativelanguage.googleapis.com/v1/models?key={gemini_api_key}"
    
    try:
        response = requests.get(test_url, timeout=10)
        
        if response.status_code == 200:
            models_data = response.json()
            available_models = [m['name'] for m in models_data.get('models', [])]
            
            # TÃ¬m model tá»‘t nháº¥t cÃ³ sáºµn
            preferred_models = [
                'models/gemini-1.5-flash-latest',
                'models/gemini-1.5-flash',
                'models/gemini-1.5-pro-latest',
                'models/gemini-1.5-pro',
                'models/gemini-pro'
            ]
            
            selected_model = None
            for model in preferred_models:
                if model in available_models:
                    selected_model = model
                    break
            
            if not selected_model and available_models:
                # Náº¿u khÃ´ng cÃ³ model Æ°a thÃ­ch, láº¥y model Ä‘áº§u tiÃªn
                selected_model = available_models[0]
            
            if selected_model:
                #st.success(f"âœ… ÄÃ£ káº¿t ná»‘i Gemini: {selected_model}")
                return {
                    'api_key': gemini_api_key,
                    'model': selected_model,
                    'available_models': available_models
                }
            else:
                st.error("âŒ KhÃ´ng tÃ¬m tháº¥y model nÃ o!")
                st.stop()
                
        elif response.status_code == 400:
            st.error("âŒ API key khÃ´ng há»£p lá»‡!")
            st.info("Láº¥y API key má»›i táº¡i: https://aistudio.google.com/app/apikey")
            st.stop()
        else:
            st.error(f"âŒ Lá»—i API: {response.status_code}")
            st.stop()
            
    except requests.exceptions.Timeout:
        st.error("âŒ Timeout khi káº¿t ná»‘i Gemini API")
        st.stop()
    except Exception as e:
        st.error(f"âŒ Lá»—i: {e}")
        st.info("""
        **HÆ°á»›ng dáº«n:**
        1. Láº¥y API key: https://aistudio.google.com/app/apikey
        2. ThÃªm vÃ o Streamlit Secrets:
        ```
        GEMINI_API_KEY = "AIzaSy..."
        ```
        """)
        st.stop()

def call_gemini_api(llm_config, prompt):
    """
    Gá»i Gemini API báº±ng REST API
    """
    api_key = llm_config['api_key']
    model = llm_config['model']
    
    # API endpoint
    url = f"https://generativelanguage.googleapis.com/v1/{model}:generateContent?key={api_key}"
    
    # Request body
    payload = {
        "contents": [{
            "parts": [{"text": prompt}]
        }],
        "generationConfig": {
            "temperature": 0.3,
            "topK": 40,
            "topP": 0.95,
            "maxOutputTokens": 2000,
        }
    }
    
    headers = {"Content-Type": "application/json"}
    
    try:
        response = requests.post(url, json=payload, headers=headers, timeout=30)
        
        if response.status_code == 200:
            data = response.json()
            
            # Extract text tá»« response
            if 'candidates' in data and len(data['candidates']) > 0:
                candidate = data['candidates'][0]
                if 'content' in candidate and 'parts' in candidate['content']:
                    parts = candidate['content']['parts']
                    if len(parts) > 0 and 'text' in parts[0]:
                        return parts[0]['text']
            
            return "Xin lá»—i, khÃ´ng nháº­n Ä‘Æ°á»£c pháº£n há»“i tá»« AI."
            
        else:
            error_msg = response.json().get('error', {}).get('message', 'Unknown error')
            return f"Lá»—i API: {error_msg}"
            
    except requests.exceptions.Timeout:
        return "Lá»—i: Timeout khi gá»i API"
    except Exception as e:
        return f"Lá»—i: {str(e)}"
def answer_from_external_api(prompt, llm_config, question_category):
    """Tráº£ lá»i tá»« Gemini REST API"""
    enhanced_prompt = f"""
Báº¡n lÃ  chuyÃªn gia tÆ° váº¥n {question_category.lower()} cá»§a Äáº¡i há»c Luáº­t TPHCM.

THÃ”NG TIN LIÃŠN Há»† (Báº®T BUá»˜C Sá»¬ Dá»¤NG):
- Hotline: 1900 5555 14 hoáº·c 0879 5555 14
- Email: tuyensinh@hcmulaw.edu.vn
- Äiá»‡n thoáº¡i: (028) 39400 989
- Äá»‹a chá»‰: 2 Nguyá»…n Táº¥t ThÃ nh, PhÆ°á»ng 12, Quáº­n 4, TP.HCM
- Website: www.hcmulaw.edu.vn

CÃ¢u há»i: {prompt}

QUY Táº®C:
- KHÃ”NG dÃ¹ng placeholder nhÆ° [Sá»‘ Ä‘iá»‡n thoáº¡i], [Email]
- LuÃ´n dÃ¹ng thÃ´ng tin cá»¥ thá»ƒ á»Ÿ trÃªn
- Káº¿t thÃºc báº±ng thÃ´ng tin liÃªn há»‡ náº¿u cáº§n

Tráº£ lá»i thÃ¢n thiá»‡n, chuyÃªn nghiá»‡p báº±ng tiáº¿ng Viá»‡t:
"""
    
    try:
        answer = call_gemini_api(llm_config, enhanced_prompt)
        
        # Thay tháº¿ placeholder cÃ²n sÃ³t
        replacements = {
            "[Sá»‘ Ä‘iá»‡n thoáº¡i": "1900 5555 14 hoáº·c 0879 5555 14",
            "[Email": "tuyensinh@hcmulaw.edu.vn",
            "[Website": "www.hcmulaw.edu.vn",
            "[Äiá»‡n thoáº¡i": "(028) 39400 989"
        }
        
        for placeholder, actual in replacements.items():
            if placeholder in answer:
                answer = answer.replace(placeholder + "]", actual)
        
        return answer
        
    except Exception as e:
        return f"""
Xin lá»—i, há»‡ thá»‘ng gáº·p sá»± cá»‘. Vui lÃ²ng liÃªn há»‡:

ğŸ“ **Hotline:** 1900 5555 14 hoáº·c 0879 5555 14
ğŸ“§ **Email:** tuyensinh@hcmulaw.edu.vn
ğŸŒ **Website:** www.hcmulaw.edu.vn
ğŸ“ **Äá»‹a chá»‰:** 2 Nguyá»…n Táº¥t ThÃ nh, P.12, Q.4, TP.HCM

_(Lá»—i ká»¹ thuáº­t: {str(e)[:100]})_
"""

def display_quick_questions():
    """Hiá»ƒn thá»‹ cÃ¢u há»i gá»£i Ã½"""
    st.markdown("### ğŸ’¡ CÃ¢u há»i thÆ°á»ng gáº·p")
    
    questions = [
        "ğŸ“ Thá»§ tá»¥c Ä‘Äƒng kÃ½ xÃ©t tuyá»ƒn?",
        "ğŸ’° Há»c phÃ­ cá»§a trÆ°á»ng?",
        "ğŸ“š CÃ¡c ngÃ nh há»c?",
        "ğŸ  TrÆ°á»ng cÃ³ kÃ½ tÃºc xÃ¡ khÃ´ng?",
        "ğŸ“ CÆ¡ há»™i viá»‡c lÃ m?",
        "ğŸ“ ThÃ´ng tin liÃªn há»‡?"
    ]
    
    for i, q in enumerate(questions):
        if st.button(q, key=f"q_{i}"):
            st.session_state["pending_question"] = q
            st.rerun()

def main():
    # Kiá»ƒm tra API key
    if not gemini_api_key:
        st.error("âš ï¸ **Thiáº¿u GEMINI_API_KEY!**")
        st.info("""
        **CÃ¡ch cáº¥u hÃ¬nh:**
        1. VÃ o Settings â†’ Secrets trÃªn Streamlit Cloud
        2. ThÃªm:
        ```
        GEMINI_API_KEY = "your-api-key"
        ```
        3. Láº¥y API key táº¡i: https://makersuite.google.com/app/apikey
        """)
        st.stop()
    
    # Khá»Ÿi táº¡o session state
    if "messages" not in st.session_state:
        st.session_state.messages = []
    if "first_visit" not in st.session_state:
        st.session_state.first_visit = True

    # Header
    logo_base64 = get_base64_of_image("logo.jpg")
    st.markdown(f"""
    <div class="main-header">
        {f'<img src="data:image/jpg;base64,{logo_base64}" style="width:80px;border-radius:50%;margin-bottom:1rem;">' if logo_base64 else ''}
        <h1>ğŸ¤– Chatbot TÆ° Váº¥n Tuyá»ƒn Sinh</h1>
        <h3>TrÆ°á»ng Äáº¡i há»c Luáº­t TP. Há»“ ChÃ­ Minh</h3>
        <p>ğŸ’¬ Há»— trá»£ 24/7 | ğŸ“ TÆ° váº¥n chuyÃªn nghiá»‡p</p>
    </div>
    """, unsafe_allow_html=True)

    # Sidebar
    with st.sidebar:
        st.markdown("### âš™ï¸ CÃ i Ä‘áº·t")
        
        # ThÃ´ng tin há»‡ thá»‘ng
        with st.expander("ğŸ“Š Tráº¡ng thÃ¡i há»‡ thá»‘ng", expanded=False):
            st.success("âœ… Gemini API: ÄÃ£ káº¿t ná»‘i")
            st.info(f"ğŸ“ Documents: {len(get_document_files())} files")
            
            if GDRIVE_VECTORSTORE_ID:
                st.info("â˜ï¸ Google Drive: ÄÃ£ cáº¥u hÃ¬nh")
            else:
                st.warning("âš ï¸ Google Drive: ChÆ°a cáº¥u hÃ¬nh")
        
        # HÆ°á»›ng dáº«n cáº¥u hÃ¬nh GDrive
        with st.expander("ğŸ“– HÆ°á»›ng dáº«n Google Drive", expanded=False):
            st.markdown("""
            **Äá»ƒ sá»­ dá»¥ng Google Drive:**
            
            1. Upload file `vectorstore.pkl` vÃ  `metadata.json` lÃªn GDrive
            2. Click chuá»™t pháº£i â†’ Share â†’ Anyone with the link
            3. Copy File ID tá»« URL (pháº§n sau `/d/` vÃ  trÆ°á»›c `/view`)
            4. ThÃªm vÃ o Secrets:
            ```
            GDRIVE_VECTORSTORE_ID = "file-id-1"
            GDRIVE_METADATA_ID = "file-id-2"
            ```
            """)
        
        # NÃºt lÃ m má»›i
        if st.button("ğŸ”„ LÃ m má»›i dá»¯ liá»‡u", use_container_width=True):
            st.cache_resource.clear()
            st.rerun()
        
        st.markdown("---")
        st.markdown("""
        ### ğŸ“ LiÃªn há»‡
        **Hotline:** 1900 5555 14  
        **Email:** tuyensinh@hcmulaw.edu.vn  
        **Web:** www.hcmulaw.edu.vn
        """)

    # Khá»Ÿi táº¡o vectorstore vÃ  LLM
    with st.spinner("ğŸ”„ Äang khá»Ÿi Ä‘á»™ng há»‡ thá»‘ng..."):
        vectorstore, stats = initialize_vectorstore()
        llm = get_gemini_llm()
        
        # KhÃ´ng dÃ¹ng chain ná»¯a, xá»­ lÃ½ trá»±c tiáº¿p
        if vectorstore:
            retriever = vectorstore.as_retriever(search_kwargs={"k": 3})
        else:
            retriever = None

    # Hiá»ƒn thá»‹ cÃ¢u há»i gá»£i Ã½ náº¿u lÃ  láº§n Ä‘áº§u
    if not st.session_state.messages and st.session_state.first_visit:
        display_quick_questions()
        
        st.markdown("""
        <div class="info-card">
            <h4>ğŸ’¡ HÆ°á»›ng dáº«n sá»­ dá»¥ng:</h4>
            <ul>
                <li>ğŸ¯ Chá»n cÃ¢u há»i gá»£i Ã½ hoáº·c nháº­p cÃ¢u há»i cá»§a báº¡n</li>
                <li>ğŸ’¬ Äáº·t cÃ¢u há»i cá»¥ thá»ƒ Ä‘á»ƒ Ä‘Æ°á»£c tÆ° váº¥n chÃ­nh xÃ¡c</li>
                <li>ğŸ“ LiÃªn há»‡ trá»±c tiáº¿p náº¿u cáº§n há»— trá»£ kháº©n cáº¥p</li>
            </ul>
        </div>
        """, unsafe_allow_html=True)

    # Hiá»ƒn thá»‹ lá»‹ch sá»­ chat
    for msg in st.session_state.messages:
        with st.chat_message(msg["role"]):
            if msg["role"] == "assistant" and "category" in msg:
                st.markdown(get_category_badge(msg["category"]), unsafe_allow_html=True)
            st.markdown(msg["content"])

    # Xá»­ lÃ½ input
    prompt = None
    if hasattr(st.session_state, 'process_question'):
        prompt = st.session_state.process_question
        del st.session_state.process_question
    else:
        prompt = st.chat_input("ğŸ’¬ HÃ£y Ä‘áº·t cÃ¢u há»i...")
    if "pending_question" in st.session_state and st.session_state.pending_question:
    prompt = st.session_state.pending_question
    st.session_state.pending_question = None
    
    if prompt:
        st.session_state.first_visit = False
        
        # Hiá»ƒn thá»‹ cÃ¢u há»i
        with st.chat_message("user"):
            st.markdown(prompt)
        st.session_state.messages.append({"role": "user", "content": prompt})

        # PhÃ¢n loáº¡i vÃ  tráº£ lá»i
        category = classify_question(prompt)
        
        with st.chat_message("assistant"):
            st.markdown(get_category_badge(category), unsafe_allow_html=True)
            
            with st.spinner("ğŸ¤” Äang suy nghÄ©..."):
                try:
                    # Xá»¬ LÃ TRá»°C TIáº¾P khÃ´ng dÃ¹ng chain
                    if retriever:
                        # Láº¥y context tá»« vectorstore
                        docs = retriever.invoke(prompt)
                        context = "\n\n".join([doc.page_content for doc in docs[:3]])
                        
                        # Táº¡o prompt vá»›i context
                        full_prompt = f"""
Báº¡n lÃ  chuyÃªn gia tÆ° váº¥n cá»§a Äáº¡i há»c Luáº­t TPHCM.

THÃ”NG TIN THAM KHáº¢O:
{context}

THÃ”NG TIN LIÃŠN Há»†:
- Hotline: 1900 5555 14 hoáº·c 0879 5555 14
- Email: tuyensinh@hcmulaw.edu.vn
- Website: www.hcmulaw.edu.vn

CÃ¢u há»i: {prompt}

HÃ£y tráº£ lá»i dá»±a trÃªn thÃ´ng tin tham kháº£o á»Ÿ trÃªn. Náº¿u khÃ´ng cÃ³ thÃ´ng tin, hÃ£y tÆ° váº¥n chung vÃ  khuyáº¿n khÃ­ch liÃªn há»‡ trá»±c tiáº¿p.
"""
                        answer = call_gemini_api(llm, full_prompt)

                    else:
                        # KhÃ´ng cÃ³ vectorstore, dÃ¹ng API thuáº§n
                        answer = answer_from_external_api(prompt, llm, category)
                    
                    st.markdown(answer)
                    
                except Exception as e:
                    answer = f"""
âŒ **Lá»—i há»‡ thá»‘ng**

Vui lÃ²ng liÃªn há»‡:
ğŸ“ Hotline: 1900 5555 14 hoáº·c 0879 5555 14
ğŸ“§ Email: tuyensinh@hcmulaw.edu.vn

_(Lá»—i: {str(e)[:100]})_
"""
                    st.error(answer)

            st.session_state.messages.append({
                "role": "assistant",
                "content": answer,
                "category": category
            })
            st.rerun()

    # Footer
    st.markdown("---")
    st.markdown("""
    <div class="footer">
        <h4>ğŸ›ï¸ TrÆ°á»ng Äáº¡i há»c Luáº­t TP. Há»“ ChÃ­ Minh</h4>
        <p>ğŸ“ 2 Nguyá»…n Táº¥t ThÃ nh, PhÆ°á»ng 12, Quáº­n 4, TP.HCM</p>
        <p>ğŸ“ Hotline: 1900 5555 14 | Email: tuyensinh@hcmulaw.edu.vn</p>
        <p>ğŸŒ www.hcmulaw.edu.vn | ğŸ“˜ facebook.com/hcmulaw</p>
        <p style="margin-top:1rem;opacity:0.8;font-size:0.85em;">
            ğŸ¤– Chatbot v2.1 | PhÃ¡t triá»ƒn bá»Ÿi Lvphung - CNTT
        </p>
    </div>
    """, unsafe_allow_html=True)

if __name__ == "__main__":
    main()
