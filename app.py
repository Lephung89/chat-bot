import streamlit as st
from langchain_community.document_loaders import PyPDFLoader, Docx2txtLoader, TextLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings
from langchain.chains import ConversationalRetrievalChain
from langchain_google_genai import GoogleGenerativeAI
from langchain.memory import ConversationBufferWindowMemory
from langchain.prompts import PromptTemplate
import os
import pickle
import json
from datetime import datetime
import gdown
import tempfile
import glob
from pathlib import Path
from dotenv import load_dotenv
import base64

def get_base64_of_image(path):
    """Convert image to base64 string"""
    try:
        with open(path, "rb") as img_file:
            return base64.b64encode(img_file.read()).decode()
    except Exception as e:
        return ""

# C·∫•u h√¨nh trang
st.set_page_config(
    page_title="Chatbot T∆∞ V·∫•n - ƒê·∫°i h·ªçc Lu·∫≠t TPHCM",
    page_icon="‚öñÔ∏è",
    layout="wide",
    initial_sidebar_state="expanded"
)

# CSS t·ªëi ∆∞u h∆°n
st.markdown("""
<style>
    @import url('https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap');
    
    * { font-family: 'Inter', sans-serif; }
    
    .main-header {
        background: linear-gradient(135deg, #1e3c72 0%, #2a5298 100%);
        padding: 2rem;
        border-radius: 20px;
        color: white;
        text-align: center;
        margin-bottom: 2rem;
        box-shadow: 0 10px 30px rgba(30, 60, 114, 0.3);
    }
    
    .main-header h1 {
        font-size: 2.2rem;
        font-weight: 700;
        margin: 0.5rem 0;
    }
    
    .chat-message {
        padding: 1.5rem;
        border-radius: 15px;
        margin: 1rem 0;
        box-shadow: 0 5px 15px rgba(0,0,0,0.1);
    }
    
    .user-message {
        background: linear-gradient(135deg, #e3f2fd 0%, #bbdefb 100%);
        border-left: 5px solid #2196f3;
    }
    
    .assistant-message {
        background: linear-gradient(135deg, #f3e5f5 0%, #e1bee7 100%);
        border-left: 5px solid #9c27b0;
    }
    
    .category-badge {
        display: inline-block;
        padding: 0.4rem 0.8rem;
        border-radius: 15px;
        font-size: 0.75rem;
        font-weight: 600;
        margin-bottom: 0.5rem;
    }
    
    .badge-tuyensinh { background: #e3f2fd; color: #1565c0; }
    .badge-hocphi { background: #e8f5e8; color: #2e7d32; }
    .badge-chuongtrinh { background: #f3e5f5; color: #6a1b9a; }
    
    .info-card {
        background: linear-gradient(135deg, #fff3e0 0%, #ffe0b2 100%);
        border: 1px solid #ffb74d;
        border-radius: 15px;
        padding: 1.5rem;
        margin: 1rem 0;
    }
    
    .footer {
        background: linear-gradient(135deg, #1e3c72 0%, #2a5298 100%);
        color: white;
        padding: 2rem;
        border-radius: 15px;
        margin-top: 3rem;
        text-align: center;
    }
    
    .stButton>button {
        background: linear-gradient(135deg, #2196f3 0%, #1976d2 100%);
        color: white;
        border: none;
        border-radius: 10px;
        padding: 0.5rem 1rem;
        font-weight: 500;
        transition: all 0.3s ease;
    }
    
    .stButton>button:hover {
        background: linear-gradient(135deg, #1976d2 0%, #1565c0 100%);
        transform: translateY(-2px);
    }
</style>
""", unsafe_allow_html=True)

# Load bi·∫øn m√¥i tr∆∞·ªùng
load_dotenv()
gemini_api_key = st.secrets.get("GEMINI_API_KEY") or os.getenv("GEMINI_API_KEY")
GDRIVE_VECTORSTORE_ID = st.secrets.get("GDRIVE_VECTORSTORE_ID") or os.getenv("GDRIVE_VECTORSTORE_ID")
GDRIVE_METADATA_ID = st.secrets.get("GDRIVE_METADATA_ID") or os.getenv("GDRIVE_METADATA_ID")

# C·∫•u h√¨nh ƒë∆∞·ªùng d·∫´n
DOCUMENTS_PATH = "documents"
VECTORSTORE_PATH = "vectorstore"

for path in [DOCUMENTS_PATH, VECTORSTORE_PATH]:
    Path(path).mkdir(exist_ok=True)

# Template prompt
COUNSELING_PROMPT_TEMPLATE = """
B·∫°n l√† chuy√™n gia t∆∞ v·∫•n tuy·ªÉn sinh Tr∆∞·ªùng ƒê·∫°i h·ªçc Lu·∫≠t Th√†nh ph·ªë H·ªì Ch√≠ Minh.

TH√îNG TIN LI√äN H·ªÜ CH√çNH TH·ª®C:
- Hotline tuy·ªÉn sinh: 1900 5555 14 ho·∫∑c 0879 5555 14
- Email: tuyensinh@hcmulaw.edu.vn
- ƒêi·ªán tho·∫°i: (028) 39400 989
- ƒê·ªãa ch·ªâ: 2 Nguy·ªÖn T·∫•t Th√†nh, Ph∆∞·ªùng 12, Qu·∫≠n 4, TP.HCM
- Website: www.hcmulaw.edu.vn

Nguy√™n t·∫Øc tr·∫£ l·ªùi:
1. Th√¢n thi·ªán, chuy√™n nghi·ªáp
2. Cung c·∫•p th√¥ng tin ch√≠nh x√°c v·ªÅ ƒê·∫°i h·ªçc Lu·∫≠t TPHCM
3. KH√îNG s·ª≠ d·ª•ng placeholder, lu√¥n d√πng th√¥ng tin li√™n h·ªá c·ª• th·ªÉ ·ªü tr√™n
4. N·∫øu kh√¥ng ch·∫Øc ch·∫Øn, khuy·∫øn kh√≠ch li√™n h·ªá tr·ª±c ti·∫øp

Th√¥ng tin tham kh·∫£o: {context}
L·ªãch s·ª≠ h·ªôi tho·∫°i: {chat_history}
C√¢u h·ªèi: {question}

Tr·∫£ l·ªùi (ti·∫øng Vi·ªát):
"""

@st.cache_resource
def load_embeddings():
    return HuggingFaceEmbeddings(
        model_name="keepitreal/vietnamese-sbert",
        model_kwargs={'device': 'cpu'},
        encode_kwargs={'normalize_embeddings': True}
    )

embeddings = load_embeddings()

def download_from_gdrive(file_id, output_path):
    """Download file t·ª´ Google Drive"""
    try:
        url = f'https://drive.google.com/uc?id={file_id}'
        gdown.download(url, output_path, quiet=True)
        return True
    except Exception as e:
        st.warning(f"‚ö†Ô∏è Kh√¥ng th·ªÉ t·∫£i t·ª´ GDrive: {e}")
        return False

def get_document_files():
    """L·∫•y danh s√°ch file trong documents"""
    files = []
    for ext in ['*.pdf', '*.docx', '*.txt']:
        files.extend(glob.glob(os.path.join(DOCUMENTS_PATH, '**', ext), recursive=True))
    return files

def get_file_hash(file_path):
    """T·∫°o hash cho file"""
    stat = os.stat(file_path)
    return f"{stat.st_mtime}_{stat.st_size}"

def load_cached_vectorstore():
    """Load vector store t·ª´ Google Drive"""
    if not GDRIVE_VECTORSTORE_ID or not GDRIVE_METADATA_ID:
        return None, {}
    
    temp_dir = tempfile.mkdtemp()
    vectorstore_path = os.path.join(temp_dir, "vectorstore.pkl")
    metadata_path = os.path.join(temp_dir, "metadata.json")
    
    try:
        if not download_from_gdrive(GDRIVE_VECTORSTORE_ID, vectorstore_path):
            return None, {}
        if not download_from_gdrive(GDRIVE_METADATA_ID, metadata_path):
            return None, {}
        
        with open(vectorstore_path, 'rb') as f:
            vectorstore = pickle.load(f)
        
        with open(metadata_path, 'r', encoding='utf-8') as f:
            metadata = json.load(f)
        
        # Cleanup
        os.remove(vectorstore_path)
        os.remove(metadata_path)
        os.rmdir(temp_dir)
        
        return vectorstore, metadata
        
    except Exception as e:
        st.error(f"‚ùå L·ªói load vectorstore: {e}")
        return None, {}

def process_documents(file_paths):
    """X·ª≠ l√Ω documents"""
    documents = []
    processed = []
    failed = []
    
    for file_path in file_paths:
        try:
            ext = Path(file_path).suffix.lower()
            
            if ext == ".pdf":
                loader = PyPDFLoader(file_path)
            elif ext == ".docx":
                loader = Docx2txtLoader(file_path)
            elif ext == ".txt":
                loader = TextLoader(file_path, encoding='utf-8')
            else:
                failed.append(f"{file_path} (kh√¥ng h·ªó tr·ª£)")
                continue
            
            docs = loader.load()
            for doc in docs:
                doc.metadata['source_file'] = os.path.basename(file_path)
                doc.metadata['processed_time'] = datetime.now().isoformat()
            
            documents.extend(docs)
            processed.append(file_path)
            
        except Exception as e:
            failed.append(f"{file_path} ({str(e)})")
    
    return documents, processed, failed

def create_vector_store(documents):
    """T·∫°o vector store"""
    if not documents:
        return None
    
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=200,
        separators=['\n\n', '\n', '.', '!', '?', ';', ':', ' ']
    )
    texts = text_splitter.split_documents(documents)
    texts = [t for t in texts if len(t.page_content.strip()) > 50]
    
    if not texts:
        return None
    
    return FAISS.from_documents(texts, embeddings)

@st.cache_resource
def initialize_vectorstore():
    """Kh·ªüi t·∫°o vectorstore"""
    # Th·ª≠ load t·ª´ cache tr∆∞·ªõc
    vectorstore, metadata = load_cached_vectorstore()
    if vectorstore:
        return vectorstore, metadata.get('stats', {})
    
    # N·∫øu kh√¥ng c√≥ cache, x·ª≠ l√Ω local files
    current_files = get_document_files()
    if not current_files:
        st.warning("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y file n√†o")
        return None, {}
    
    with st.spinner("üîÑ ƒêang x·ª≠ l√Ω t√†i li·ªáu..."):
        documents, processed, failed = process_documents(current_files)
        
        if not documents:
            st.error("‚ùå Kh√¥ng th·ªÉ x·ª≠ l√Ω file")
            return None, {}
        
        vectorstore = create_vector_store(documents)
        
        if vectorstore:
            stats = {
                'total_files': len(current_files),
                'processed_files': len(processed),
                'failed_files': len(failed),
                'total_chunks': vectorstore.index.ntotal,
                'last_updated': datetime.now().isoformat()
            }
            return vectorstore, stats
    
    return None, {}

def classify_question(question):
    """Ph√¢n lo·∫°i c√¢u h·ªèi"""
    question_lower = question.lower()
    
    categories = {
        "Tuy·ªÉn sinh": ["tuy·ªÉn sinh", "ƒëƒÉng k√Ω", "h·ªì s∆°", "ƒëi·ªÉm chu·∫©n", "x√©t tuy·ªÉn"],
        "H·ªçc ph√≠": ["h·ªçc ph√≠", "chi ph√≠", "mi·ªÖn gi·∫£m", "h·ªçc b·ªïng"],
        "Ch∆∞∆°ng tr√¨nh ƒë√†o t·∫°o": ["ch∆∞∆°ng tr√¨nh", "m√¥n h·ªçc", "t√≠n ch·ªâ", "ng√†nh"],
    }
    
    for category, keywords in categories.items():
        if any(kw in question_lower for kw in keywords):
            return category
    return "Kh√°c"

def get_category_badge(category):
    """T·∫°o badge cho category"""
    badge_map = {
        "Tuy·ªÉn sinh": "badge-tuyensinh",
        "H·ªçc ph√≠": "badge-hocphi",
        "Ch∆∞∆°ng tr√¨nh ƒë√†o t·∫°o": "badge-chuongtrinh"
    }
    badge_class = badge_map.get(category, "badge-tuyensinh")
    return f'<span class="category-badge {badge_class}">{category}</span>'

def create_conversational_chain(vector_store, llm):
    """T·∫°o chain"""
    prompt = PromptTemplate(
        template=COUNSELING_PROMPT_TEMPLATE,
        input_variables=["context", "chat_history", "question"]
    )
    
    memory = ConversationBufferWindowMemory(
        k=5,
        memory_key="chat_history",
        return_messages=True,
        output_key="answer"
    )
    
    return ConversationalRetrievalChain.from_llm(
        llm=llm,
        retriever=vector_store.as_retriever(search_kwargs={"k": 5}),
        memory=memory,
        return_source_documents=True,
        combine_docs_chain_kwargs={"prompt": prompt}
    )

@st.cache_resource
def get_gemini_llm():
    """Kh·ªüi t·∫°o Gemini LLM v·ªõi c·∫•u h√¨nh t·ªëi ∆∞u"""
    try:
        return GoogleGenerativeAI(
            model="gemini-1.5-flash",  # Ho·∫∑c th·ª≠ "models/gemini-1.5-flash"
            google_api_key=gemini_api_key,
            temperature=0.3,
            max_output_tokens=2000
        )
    except Exception as e:
        st.error(f"‚ùå L·ªói kh·ªüi t·∫°o Gemini: {e}")
        # Fallback sang model kh√°c n·∫øu c·∫ßn
        return GoogleGenerativeAI(
            model="gemini-pro",
            google_api_key=gemini_api_key,
            temperature=0.3
        )

def answer_from_external_api(prompt, llm, question_category):
    """Tr·∫£ l·ªùi t·ª´ API"""
    enhanced_prompt = f"""
B·∫°n l√† chuy√™n gia t∆∞ v·∫•n {question_category.lower()} c·ªßa ƒê·∫°i h·ªçc Lu·∫≠t TPHCM.

TH√îNG TIN LI√äN H·ªÜ (B·∫ÆT BU·ªòC S·ª¨ D·ª§NG):
- Hotline: 1900 5555 14 ho·∫∑c 0879 5555 14
- Email: tuyensinh@hcmulaw.edu.vn
- ƒêi·ªán tho·∫°i: (028) 39400 989
- ƒê·ªãa ch·ªâ: 2 Nguy·ªÖn T·∫•t Th√†nh, Ph∆∞·ªùng 12, Qu·∫≠n 4, TP.HCM
- Website: www.hcmulaw.edu.vn

C√¢u h·ªèi: {prompt}

QUY T·∫ÆC:
- KH√îNG d√πng placeholder nh∆∞ [S·ªë ƒëi·ªán tho·∫°i], [Email]
- Lu√¥n d√πng th√¥ng tin c·ª• th·ªÉ ·ªü tr√™n
- K·∫øt th√∫c b·∫±ng th√¥ng tin li√™n h·ªá n·∫øu c·∫ßn

Tr·∫£ l·ªùi th√¢n thi·ªán, chuy√™n nghi·ªáp:
"""
    
    try:
        response = llm.invoke(enhanced_prompt)
        
        # Thay th·∫ø placeholder c√≤n s√≥t
        replacements = {
            "[S·ªë ƒëi·ªán tho·∫°i": "1900 5555 14 ho·∫∑c 0879 5555 14",
            "[Email": "tuyensinh@hcmulaw.edu.vn",
            "[Website": "www.hcmulaw.edu.vn"
        }
        
        for placeholder, actual in replacements.items():
            if placeholder in response:
                response = response.replace(placeholder + "]", actual)
        
        return response
        
    except Exception as e:
        return f"""
Xin l·ªói, h·ªá th·ªëng g·∫∑p s·ª± c·ªë. Vui l√≤ng li√™n h·ªá:

üìû Hotline: 1900 5555 14 ho·∫∑c 0879 5555 14
üìß Email: tuyensinh@hcmulaw.edu.vn
üåê Website: www.hcmulaw.edu.vn
üìç ƒê·ªãa ch·ªâ: 2 Nguy·ªÖn T·∫•t Th√†nh, P.12, Q.4, TP.HCM

L·ªói: {str(e)}
"""

def display_quick_questions():
    """Hi·ªÉn th·ªã c√¢u h·ªèi g·ª£i √Ω"""
    st.markdown("### üí° C√¢u h·ªèi th∆∞·ªùng g·∫∑p")
    
    questions = [
        "üìù Th·ªß t·ª•c ƒëƒÉng k√Ω x√©t tuy·ªÉn?",
        "üí∞ H·ªçc ph√≠ c·ªßa tr∆∞·ªùng?",
        "üìö C√°c ng√†nh h·ªçc?",
        "üè† Tr∆∞·ªùng c√≥ k√Ω t√∫c x√° kh√¥ng?",
        "üéì C∆° h·ªôi vi·ªác l√†m?",
        "üìû Th√¥ng tin li√™n h·ªá?"
    ]
    
    cols = st.columns(2)
    for i, q in enumerate(questions):
        with cols[i % 2]:
            if st.button(q, key=f"q_{i}", use_container_width=True):
                clean_q = q.split(" ", 1)[1]
                st.session_state.messages.append({"role": "user", "content": clean_q})
                st.session_state.process_question = clean_q
                st.rerun()

def main():
    # Ki·ªÉm tra API key
    if not gemini_api_key:
        st.error("‚ö†Ô∏è Thi·∫øu GEMINI_API_KEY! Vui l√≤ng c·∫•u h√¨nh trong Streamlit Secrets")
        st.stop()
    
    # Kh·ªüi t·∫°o session state
    if "messages" not in st.session_state:
        st.session_state.messages = []
    if "first_visit" not in st.session_state:
        st.session_state.first_visit = True

    # Header
    logo_base64 = get_base64_of_image("logo.jpg")
    st.markdown(f"""
    <div class="main-header">
        {f'<img src="data:image/jpg;base64,{logo_base64}" style="width:80px;border-radius:50%;margin-bottom:1rem;">' if logo_base64 else ''}
        <h1>ü§ñ Chatbot T∆∞ V·∫•n Tuy·ªÉn Sinh</h1>
        <h3>Tr∆∞·ªùng ƒê·∫°i h·ªçc Lu·∫≠t TP. H·ªì Ch√≠ Minh</h3>
        <p>üí¨ H·ªó tr·ª£ 24/7 | üéì T∆∞ v·∫•n chuy√™n nghi·ªáp</p>
    </div>
    """, unsafe_allow_html=True)

    # Sidebar
    with st.sidebar:
        st.markdown("### ‚öôÔ∏è C√†i ƒë·∫∑t")
        
        # Th√¥ng tin h·ªá th·ªëng
        with st.expander("üìä Th√¥ng tin h·ªá th·ªëng", expanded=False):
            st.info(f"""
            **Tr·∫°ng th√°i:**
            - ‚úÖ Gemini API: {'ƒê√£ k·∫øt n·ªëi' if gemini_api_key else '‚ùå Ch∆∞a c·∫•u h√¨nh'}
            - üìÅ Documents: {len(get_document_files())} files
            """)
        
        # N√∫t l√†m m·ªõi
        if st.button("üîÑ L√†m m·ªõi d·ªØ li·ªáu", use_container_width=True):
            st.cache_resource.clear()
            st.rerun()
        
        st.markdown("---")
        st.markdown("""
        ### üìû Li√™n h·ªá
        **Hotline:** 1900 5555 14  
        **Email:** tuyensinh@hcmulaw.edu.vn  
        **Web:** www.hcmulaw.edu.vn
        """)

    # Kh·ªüi t·∫°o vectorstore
    with st.spinner("üîÑ ƒêang kh·ªüi t·∫°o h·ªá th·ªëng..."):
        vectorstore, stats = initialize_vectorstore()
        llm = get_gemini_llm()
        chain = create_conversational_chain(vectorstore, llm) if vectorstore else None

    # Hi·ªÉn th·ªã c√¢u h·ªèi g·ª£i √Ω n·∫øu l√† l·∫ßn ƒë·∫ßu
    if not st.session_state.messages and st.session_state.first_visit:
        display_quick_questions()
        
        st.markdown("""
        <div class="info-card">
            <h4>üí° H∆∞·ªõng d·∫´n s·ª≠ d·ª•ng:</h4>
            <ul>
                <li>üéØ Ch·ªçn c√¢u h·ªèi g·ª£i √Ω ho·∫∑c nh·∫≠p c√¢u h·ªèi c·ªßa b·∫°n</li>
                <li>üí¨ ƒê·∫∑t c√¢u h·ªèi c·ª• th·ªÉ ƒë·ªÉ ƒë∆∞·ª£c t∆∞ v·∫•n ch√≠nh x√°c</li>
                <li>üìû Li√™n h·ªá tr·ª±c ti·∫øp n·∫øu c·∫ßn h·ªó tr·ª£ kh·∫©n c·∫•p</li>
            </ul>
        </div>
        """, unsafe_allow_html=True)

    # Hi·ªÉn th·ªã l·ªãch s·ª≠ chat
    for msg in st.session_state.messages:
        with st.chat_message(msg["role"]):
            if msg["role"] == "assistant" and "category" in msg:
                st.markdown(get_category_badge(msg["category"]), unsafe_allow_html=True)
            st.markdown(msg["content"])

    # X·ª≠ l√Ω input
    prompt = None
    if hasattr(st.session_state, 'process_question'):
        prompt = st.session_state.process_question
        del st.session_state.process_question
    else:
        prompt = st.chat_input("üí¨ H√£y ƒë·∫∑t c√¢u h·ªèi...")

    if prompt:
        st.session_state.first_visit = False
        
        # Hi·ªÉn th·ªã c√¢u h·ªèi
        with st.chat_message("user"):
            st.markdown(prompt)
        st.session_state.messages.append({"role": "user", "content": prompt})

        # Ph√¢n lo·∫°i v√† tr·∫£ l·ªùi
        category = classify_question(prompt)
        
        with st.chat_message("assistant"):
            st.markdown(get_category_badge(category), unsafe_allow_html=True)
            
            with st.spinner("ü§î ƒêang suy nghƒ©..."):
                try:
                    if chain:
                        response = chain({"question": prompt})
                        answer = response["answer"]
                    else:
                        answer = answer_from_external_api(prompt, llm, category)
                    
                    st.markdown(answer)
                    
                except Exception as e:
                    answer = f"""
‚ùå **L·ªói h·ªá th·ªëng**

Vui l√≤ng li√™n h·ªá:
üìû Hotline: 1900 5555 14 ho·∫∑c 0879 5555 14
üìß Email: tuyensinh@hcmulaw.edu.vn

L·ªói: {str(e)}
"""
                    st.error(answer)

            st.session_state.messages.append({
                "role": "assistant",
                "content": answer,
                "category": category
            })

    # Footer
    st.markdown("---")
    st.markdown("""
    <div class="footer">
        <h4>üèõÔ∏è Tr∆∞·ªùng ƒê·∫°i h·ªçc Lu·∫≠t TP. H·ªì Ch√≠ Minh</h4>
        <p>üìç 2 Nguy·ªÖn T·∫•t Th√†nh, Ph∆∞·ªùng 12, Qu·∫≠n 4, TP.HCM</p>
        <p>üìû Hotline: 1900 5555 14 | Email: tuyensinh@hcmulaw.edu.vn</p>
        <p>üåê www.hcmulaw.edu.vn | üìò facebook.com/hcmulaw</p>
        <p style="margin-top:1rem;opacity:0.8;font-size:0.85em;">
            ü§ñ Chatbot v2.0 | Ph√°t tri·ªÉn b·ªüi Lvphung - CNTT
        </p>
    </div>
    """, unsafe_allow_html=True)

if __name__ == "__main__":
    main()
